{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aGuLJnq_ZQxZ"
   },
   "source": [
    "#Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6vB_EEgJI_zd"
   },
   "outputs": [],
   "source": [
    "# Special thanks to Christine Chen for coding this!\n",
    "!pip install bert-serving-client\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path, PurePath\n",
    "from transformers import *\n",
    "from summarizer import Summarizer\n",
    "import logging\n",
    "import torch\n",
    "from numpy import ndarray\n",
    "from typing import List\n",
    "from tqdm.notebook import tqdm\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g2SvvbKEUtR8"
   },
   "outputs": [],
   "source": [
    "# set data paths, this requires local drive to have a folder calld \"COVID-19\" with the metadata.csv file\n",
    "# returns a string to the local path setup\n",
    "def setup_local_data():\n",
    "    input_dir = \"./Cord-2\"\n",
    "    return input_dir\n",
    "nlp = spacy.load('/home/acorn/Downloads/en_core_sci_lg-0.2.4/en_core_sci_lg/en_core_sci_lg-0.2.4/', disable=[\"ner\",\"tagger\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RxETV-CSU2ob"
   },
   "outputs": [],
   "source": [
    "#read the metadata file into df\n",
    "def read_metadata_csv(input_dir):\n",
    "    metadata_path = input_dir  + '/metadata_v5.csv'\n",
    "    metadata = pd.read_csv(metadata_path, \n",
    "                         dtype={'title':str,\n",
    "                                'abstract':str})\n",
    "    #set the abstract to the paper title if it is null\n",
    "    metadata['abstract'] = metadata['abstract'].fillna(metadata['title'])\n",
    "    #remove if abstract is empty or contains only one word\n",
    "    metadata = metadata.dropna(subset=['abstract'], axis = 0)\n",
    "    metadata['number_tokens'] = metadata['abstract'].apply(lambda x: len(x.split()))\n",
    "    metadata = metadata[metadata['number_tokens']>1].reset_index(drop=True)\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rGUOHfQiLLal"
   },
   "outputs": [],
   "source": [
    "def create_custom_model_tokenizer(pretrain_model):\n",
    "    # Load model, model config and tokenizer via Transformers\n",
    "    custom_config = AutoConfig.from_pretrained(pretrain_model)\n",
    "    custom_config.output_hidden_states=True\n",
    "    custom_tokenizer = AutoTokenizer.from_pretrained(pretrain_model)\n",
    "    custom_model = AutoModel.from_pretrained(pretrain_model, config=custom_config)\n",
    "    return custom_model, custom_tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vmdk0PnmriZ9"
   },
   "outputs": [],
   "source": [
    "# from spacy.lang.en import English\n",
    "\n",
    "# class SentenceHandler(object):\n",
    "\n",
    "#     def __init__(self, language = English):\n",
    "#         self.nlp = language()\n",
    "#         self.nlp.add_pipe(self.nlp.create_pipe('sentencizer'))\n",
    "\n",
    "#     def process(self, body: str, min_length: int = 40, max_length: int = 600):\n",
    "#         \"\"\"\n",
    "#         Processes the content sentences.\n",
    "#         :param body: The raw string body to process\n",
    "#         :param min_length: Minimum length that the sentences must be\n",
    "#         :param max_length: Max length that the sentences mus fall under\n",
    "#         :return: Returns a list of sentences.\n",
    "#         \"\"\"\n",
    "#         doc = self.nlp(body)\n",
    "#         doc.is_parsed=True\n",
    "#         return [c.string.strip() for c in doc.sents if max_length > len(c.string.strip()) > min_length]\n",
    "\n",
    "#     def __call__(self, body: str, min_length: int = 40, max_length: int = 600):\n",
    "#         return self.process(body, min_length, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7XOWAoSoWaSs"
   },
   "outputs": [],
   "source": [
    "def extract_summary(text, custom_model=None, custom_tokenizer = None):\n",
    "    model = Summarizer(custom_model=custom_model, custom_tokenizer=custom_tokenizer)\n",
    "    return model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-5-H1QxrWfp7"
   },
   "outputs": [],
   "source": [
    "#instantiate custom model and tokenizer\n",
    "sciBert, sciBert_tokenizer = create_custom_model_tokenizer('/home/acorn/Downloads/scibert_scivocab_uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QE41ayAFUhL_"
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "\n",
    "class BertParent(object):\n",
    "\n",
    "    \"\"\"\n",
    "    Base handler for BERT models.\n",
    "    \"\"\"\n",
    "\n",
    "    MODELS = {\n",
    "        'bert-base-uncased': (BertModel, BertTokenizer),\n",
    "        'bert-large-uncased': (BertModel, BertTokenizer),\n",
    "        'xlnet-base-cased': (XLNetModel, XLNetTokenizer),\n",
    "        'xlm-mlm-enfr-1024': (XLMModel, XLMTokenizer),\n",
    "        'distilbert-base-uncased': (DistilBertModel, DistilBertTokenizer),\n",
    "        'albert-base-v1': (AlbertModel, AlbertTokenizer),\n",
    "        'albert-large-v1': (AlbertModel, AlbertTokenizer),\n",
    "        ## added this to extract sciBert embeddings\n",
    "        'allenai/scibert_scivocab_uncased': (sciBert, sciBert_tokenizer) \n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str,\n",
    "        custom_model: PreTrainedModel=None,\n",
    "        custom_tokenizer: PreTrainedTokenizer=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param model: Model is the string path for the bert weights. If given a keyword, the s3 path will be used\n",
    "        :param custom_model: This is optional if a custom bert model is used\n",
    "        :param custom_tokenizer: Place to use custom tokenizer\n",
    "        \"\"\"\n",
    "\n",
    "        base_model, base_tokenizer = self.MODELS.get(model, (None, None))\n",
    "\n",
    "        if custom_model:\n",
    "            self.model = custom_model\n",
    "        else:\n",
    "            self.model = base_model.from_pretrained(model, output_hidden_states=True)\n",
    "\n",
    "        if custom_tokenizer:\n",
    "            self.tokenizer = custom_tokenizer\n",
    "        else:\n",
    "            self.tokenizer = base_tokenizer.from_pretrained(model)\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "    def tokenize_input(self, text: str) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Tokenizes the text input.\n",
    "        :param text: Text to tokenize\n",
    "        :return: Returns a torch tensor\n",
    "        \"\"\"\n",
    "        tokenized_text = self.tokenizer.tokenize(text)\n",
    "        indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "        return torch.tensor([indexed_tokens])\n",
    "\n",
    "    def extract_embeddings(\n",
    "        self,\n",
    "        text: str,\n",
    "        hidden: int=-2,\n",
    "        squeeze: bool=False,\n",
    "        reduce_option: str ='mean'\n",
    "    ) -> ndarray:\n",
    "\n",
    "        \"\"\"\n",
    "        Extracts the embeddings for the given text\n",
    "        :param text: The text to extract embeddings for.\n",
    "        :param hidden: The hidden layer to use for a readout handler\n",
    "        :param squeeze: If we should squeeze the outputs (required for some layers)\n",
    "        :param reduce_option: How we should reduce the items.\n",
    "        :return: A numpy array.\n",
    "        \"\"\"\n",
    "\n",
    "        tokens_tensor = self.tokenize_input(text)\n",
    "        pooled, hidden_states = self.model(tokens_tensor)[-2:]\n",
    "\n",
    "        if -1 > hidden > -12:\n",
    "\n",
    "            if reduce_option == 'max':\n",
    "                pooled = hidden_states[hidden].max(dim=1)[0]\n",
    "\n",
    "            elif reduce_option == 'median':\n",
    "                pooled = hidden_states[hidden].median(dim=1)[0]\n",
    "\n",
    "            else:\n",
    "                pooled = hidden_states[hidden].mean(dim=1)\n",
    "\n",
    "        if squeeze:\n",
    "            return pooled.detach().numpy().squeeze()\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def create_matrix(\n",
    "        self,\n",
    "        content: List[str],\n",
    "        hidden: int=-2,\n",
    "        reduce_option: str = 'mean'\n",
    "    ) -> ndarray:\n",
    "        \"\"\"\n",
    "        Create matrix from the embeddings\n",
    "        :param content: The list of sentences\n",
    "        :param hidden: Which hidden layer to use\n",
    "        :param reduce_option: The reduce option to run.\n",
    "        :return: A numpy array matrix of the given content.\n",
    "        \"\"\"\n",
    "\n",
    "        return np.asarray([\n",
    "            np.squeeze(self.extract_embeddings(t, hidden=hidden, reduce_option=reduce_option).data.numpy())\n",
    "            for t in content\n",
    "        ])\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        content: List[str],\n",
    "        hidden: int= -2,\n",
    "        reduce_option: str = 'mean'\n",
    "    ) -> ndarray:\n",
    "        return self.create_matrix(content, hidden, reduce_option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SRzYGzMhY6Os"
   },
   "outputs": [],
   "source": [
    "#BertParent.MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3KsiSQpsYklK"
   },
   "outputs": [],
   "source": [
    "def get_summary_embeddings(summary, model):\n",
    "    emb = BertParent(model=model)\n",
    "    return emb.extract_embeddings(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tM47x-E_Zap2"
   },
   "source": [
    "#Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "colab_type": "code",
    "id": "vSNibiWbJjUu",
    "outputId": "7375a2cc-fd3f-493f-8be4-bec21a055c74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 47298 entries, 0 to 47297\n",
      "Data columns (total 18 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   cord_uid                     47298 non-null  object \n",
      " 1   sha                          34283 non-null  object \n",
      " 2   source_x                     47298 non-null  object \n",
      " 3   title                        47140 non-null  object \n",
      " 4   doi                          43956 non-null  object \n",
      " 5   pmcid                        28038 non-null  object \n",
      " 6   pubmed_id                    35409 non-null  float64\n",
      " 7   license                      47298 non-null  object \n",
      " 8   abstract                     39048 non-null  object \n",
      " 9   publish_time                 47289 non-null  object \n",
      " 10  authors                      45189 non-null  object \n",
      " 11  journal                      42894 non-null  object \n",
      " 12  Microsoft Academic Paper ID  964 non-null    float64\n",
      " 13  WHO #Covidence               1768 non-null   object \n",
      " 14  has_pdf_parse                47298 non-null  bool   \n",
      " 15  has_pmc_xml_parse            47298 non-null  bool   \n",
      " 16  full_text_file               38469 non-null  object \n",
      " 17  url                          46996 non-null  object \n",
      "dtypes: bool(2), float64(2), object(14)\n",
      "memory usage: 5.9+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acorn/anaconda3/envs/summarize/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (13) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "local_dir = setup_local_data()\n",
    "metadata = pd.read_csv(\"./CORD-19-research-challenge/metadata.csv\")\n",
    "metadata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "olwcBuJWEsNA"
   },
   "outputs": [],
   "source": [
    "CHUNKS_COUNT = 20\n",
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "5c8eda0f77af4f1e8cf7a204a377a6e4",
      "08a1cf4bd2354c42af633ce196ca8cf2",
      "ad5bd10337394d659b01f978557bbe37",
      "c2a2886423e24f24b173f7be85d50854",
      "e800bc89f1674e22bc5c9b0fb0807bf0",
      "87d56570596e46b69796c3b2663a4a6a",
      "ae9d22e2fed348289e6ae30a7f16a9f4",
      "1af6cda35b354e109e8776dbb436365d"
     ]
    },
    "colab_type": "code",
    "id": "TDtjDA-JW-l8",
    "outputId": "e99c2ce1-dbbf-445f-a9ef-ce40b25805f3"
   },
   "outputs": [],
   "source": [
    "#extract summaries from the abstracts\n",
    "#chunk processing due to long processing time and possible notebook runtime shutdowns\n",
    "chunks = np.array_split(metadata, CHUNKS_COUNT)\n",
    "chunk_dfs = []\n",
    "for index, chunk in enumerate(chunks):\n",
    "    chunk = chunk.reset_index()\n",
    "    vector_list = []\n",
    "    for i in tqdm(chunk.index):\n",
    "        if isinstance(chunk.iloc[i]['abstract'], str):\n",
    "            if len(chunk.iloc[i]['abstract']) > 10:\n",
    "                summary = extract_summary(chunk.iloc[i]['abstract'], \n",
    "                                          custom_model=sciBert, \n",
    "                                          custom_tokenizer=sciBert_tokenizer)\n",
    "                vector_list.append(\n",
    "                {\n",
    "                \"cord_uid\": chunk.iloc[i]['cord_uid'],\n",
    "                \"sha\": chunk.iloc[i]['sha'],\n",
    "                \"scibert_emb\": bc.encode([summary])[0],\n",
    "                \"summary\": summary\n",
    "                })\n",
    "            else: \n",
    "                pass\n",
    "\n",
    "vector_df = pd.DataFrame(data=vector_list)\n",
    "vector_df.to_json('abstract_summaries.json')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "name": "Copy of ExtractiveSummarizer.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "08a1cf4bd2354c42af633ce196ca8cf2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1af6cda35b354e109e8776dbb436365d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5c8eda0f77af4f1e8cf7a204a377a6e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ad5bd10337394d659b01f978557bbe37",
       "IPY_MODEL_c2a2886423e24f24b173f7be85d50854"
      ],
      "layout": "IPY_MODEL_08a1cf4bd2354c42af633ce196ca8cf2"
     }
    },
    "87d56570596e46b69796c3b2663a4a6a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad5bd10337394d659b01f978557bbe37": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "  1%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_87d56570596e46b69796c3b2663a4a6a",
      "max": 2266,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e800bc89f1674e22bc5c9b0fb0807bf0",
      "value": 15
     }
    },
    "ae9d22e2fed348289e6ae30a7f16a9f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c2a2886423e24f24b173f7be85d50854": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1af6cda35b354e109e8776dbb436365d",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_ae9d22e2fed348289e6ae30a7f16a9f4",
      "value": " 15/2266 [00:10&lt;23:16,  1.61it/s]"
     }
    },
    "e800bc89f1674e22bc5c9b0fb0807bf0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
