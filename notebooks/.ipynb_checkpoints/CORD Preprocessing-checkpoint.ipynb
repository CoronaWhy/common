{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Description\n",
    "\n",
    "Current implementation only searches JSON files recursively in the folder, convert these to a dataframe, and processes the texts in paralell. Processing details below.\n",
    "\n",
    "* Text is first labeled by language. If english:\n",
    "\n",
    "* Text acronyms are expanded. i.e. ADD --> Attention Deficit Disorder. This is done using the acronym expansion module in scispaCy (see their homepage for documentation).\n",
    "\n",
    "* Concepts (general NER) in the text are linked to the Unified Medical Language System (UMLS) and canonicalized. The first alias for the entity is appended to the UMLS column.\n",
    "\n",
    "* Text is non-destructively lemmatized. No stop words, no deletions of punctuation. For TF-IDF or other algorithms that depend on tokenization, you'll need to run a filter over this column for dimensionality reducation and cleaner text. This mean Sars-Covid-19 stays Sars-Covid-19 as a single token. If you need to match drug names, you can do full-text search on the \"sentence\" column, or attempt to match to tokens in UMLS, or match NER results in DRUG column.\n",
    "\n",
    "* A second pass on NER is run using four NER-specific models from scispaCy. \"en_ner_craft_md\", \"en_ner_jnlpba_md\",\"en_ner_bc5cdr_md\",\"en_ner_bionlp13cg_md\". For more information, please see their homepage.\n",
    "\n",
    "\n",
    "## A note on the Extraction class, and section labels\n",
    "\n",
    "* The extraction class needs to be edited to read the metadata file and choose files accordingly. Right now, this is at the top of our priority list for tasks in #datasets, and if you can help with this please PM Brandon Eychaner. \n",
    "\n",
    "* Section labels are _messy_. There are more than 250,000 unique section labels in the JSONs alone. I listed the top 1000 section labels by count and took the obvious ones, and mapped them in the \"filter_dict\" variable to account for the majority of important sections. This is an area of ongoing work. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "import numpy as np\n",
    "import scispacy\n",
    "import json\n",
    "import spacy\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.spatial import distance\n",
    "import ipywidgets as widgets\n",
    "from scispacy.abbreviation import AbbreviationDetector\n",
    "from spacy_langdetect import LanguageDetector\n",
    "# UMLS linking will find concepts in the text, and link them to UMLS. \n",
    "from scispacy.umls_linking import UmlsEntityLinker\n",
    "import time\n",
    "from spacy.vocab import Vocab\n",
    "from multiprocessing import Process, Queue, Manager\n",
    "from multiprocessing.pool import Pool\n",
    "from functools import partial\n",
    "import re\n",
    "from googletrans import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a dictionary object that's easy to parse in pandas. For tables! :D\n",
    "def extract_tables_from_json(js):\n",
    "    json_list = []\n",
    "    # Figures contain useful information. Since NLP doesn't handle images and tables,\n",
    "    # we can leverage this text data in lieu of visual data.\n",
    "    for figure in list(js[\"ref_entries\"].keys()):\n",
    "        json_dict = [\"figref\", figure, js[\"ref_entries\"][figure][\"text\"]]\n",
    "        json_dict.append(json_dict)\n",
    "    return json_list\n",
    "\n",
    "def init_filter_dict(): \n",
    "    inverse = dict() \n",
    "    d = {\n",
    "        \"discussion\": [\"conclusions\",\"conclusion\",'| discussion', \"discussion\",  'concluding remarks',\n",
    "                       'discussion and conclusions','conclusion:', 'discussion and conclusion',\n",
    "                       'conclusions:', 'outcomes', 'conclusions and perspectives', \n",
    "                       'conclusions and future perspectives', 'conclusions and future directions'],\n",
    "        \"results\": ['executive summary', 'result', 'summary','results','results and discussion','results:',\n",
    "                    'comment',\"findings\"],\n",
    "        \"introduction\": ['introduction', 'background', 'i. introduction','supporting information','| introduction'],\n",
    "        \"methods\": ['methods','method','statistical methods','materials','materials and methods',\n",
    "                    'data collection','the study','study design','experimental design','objective',\n",
    "                    'objectives','procedures','data collection and analysis', 'methodology',\n",
    "                    'material and methods','the model','experimental procedures','main text',],\n",
    "        \"statistics\": ['data analysis','statistical analysis', 'analysis','statistical analyses', \n",
    "                       'statistics','data','measures'],\n",
    "        \"clinical\": ['diagnosis', 'diagnostic features', \"differential diagnoses\", 'classical signs','prognosis', 'clinical signs', 'pathogenesis',\n",
    "                     'etiology','differential diagnosis','clinical features', 'case report', 'clinical findings',\n",
    "                     'clinical presentation'],\n",
    "        'treatment': ['treatment', 'interventions'],\n",
    "        \"prevention\": ['epidemiology','risk factors'],\n",
    "        \"subjects\": ['demographics','samples','subjects', 'study population','control','patients', \n",
    "                   'participants','patient characteristics'],\n",
    "        \"animals\": ['animals','animal models'],\n",
    "        \"abstract\": [\"abstract\", 'a b s t r a c t','author summary'], \n",
    "        \"review\": ['review','literature review','keywords']}\n",
    "    \n",
    "    for key in d: \n",
    "        # Go through the list that is saved in the dict:\n",
    "        for item in d[key]:\n",
    "            # Check if in the inverted dict the key exists\n",
    "            if item not in inverse: \n",
    "                # If not create a new list\n",
    "                inverse[item] = [key] \n",
    "            else: \n",
    "                inverse[item].append(key) \n",
    "    return inverse\n",
    "\n",
    "inverted_dict = init_dict()\n",
    "    \n",
    "def get_section_name(text):\n",
    "    if len(text) == 0:\n",
    "        return(text)\n",
    "    text = text.lower()\n",
    "    if text in inverted_dict.keys():\n",
    "        return(inverted_dict[text][0])\n",
    "    else:\n",
    "        if \"case\" in text or \"study\" in text: \n",
    "            return(\"methods\")\n",
    "        elif \"clinic\" in text:\n",
    "            return(\"clinical\")\n",
    "        elif \"stat\" in text:\n",
    "            return(\"statistics\")\n",
    "        elif \"intro\" in text or \"backg\" in text:\n",
    "            return(\"introduction\")\n",
    "        elif \"data\" in text:\n",
    "            return(\"statistics\")\n",
    "        elif \"discuss\" in text:\n",
    "            return(\"discussion\")\n",
    "        elif \"patient\" in text:\n",
    "            return(\"subjects\")\n",
    "        else: \n",
    "            return(text)\n",
    "\n",
    "def init_nlp():\n",
    "    nlp = spacy.load(\"/homlanguages[0:10]e/acorn/Downloads/en_core_sci_lg-0.2.4/en_core_sci_lg/en_core_sci_lg-0.2.4/\", disable=[\"tagger\"])\n",
    "    nlp.max_length=2000000\n",
    "\n",
    "    # We also need to detect language, or else we'll be parsing non-english text \n",
    "    # as if it were English. \n",
    "    nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\n",
    "\n",
    "    # Add the abbreviation pipe to the spacy pipeline. Only need to run this once.\n",
    "    abbreviation_pipe = AbbreviationDetector(nlp)\n",
    "    nlp.add_pipe(abbreviation_pipe)\n",
    "\n",
    "    # Our linker will look up named entities/concepts in the UMLS graph and normalize\n",
    "    # the data for us. \n",
    "    linker = UmlsEntityLinker(resolve_abbreviations=True)\n",
    "    nlp.add_pipe(linker)\n",
    "    \n",
    "    new_vector = nlp(\n",
    "               \"\"\"Positive-sense single‚Äêstranded ribonucleic acid virus, subgenus \n",
    "                   sarbecovirus of the genus Betacoronavirus. \n",
    "                   Also known as severe acute respiratory syndrome coronavirus 2, \n",
    "                   also known by 2019 novel coronavirus. It is \n",
    "                   contagious in humans and is the cause of the ongoing pandemic of \n",
    "                   coronavirus disease. Coronavirus disease 2019 is a zoonotic infectious \n",
    "                   disease.\"\"\").vector\n",
    "\n",
    "    vector_data = {\"COVID-19\": new_vector,\n",
    "               \"2019-nCoV\": new_vector,\n",
    "               \"SARS-CoV-2\": new_vector}\n",
    "\n",
    "    vocab = Vocab()\n",
    "    for word, vector in vector_data.items():\n",
    "        nlp.vocab.set_vector(word, vector)\n",
    "    \n",
    "    return(nlp, linker)\n",
    "def init_ner():\n",
    "    models = [\"en_ner_craft_md\", \"en_ner_jnlpba_md\",\"en_ner_bc5cdr_md\",\"en_ner_bionlp13cg_md\"]\n",
    "    nlps = [spacy.load(model) for model in models]\n",
    "    return(nlps)\n",
    "\n",
    "def process_metadata(directory):\n",
    "    rows = []\n",
    "    if directory[-1] != \"/\": \n",
    "        directory = directory + \"/\"\n",
    "        \n",
    "    df = pd.read_csv(directory + \"metadata\")\n",
    "    for i in df[df[\"has_pmc_xml_parse\"] == 1].index:\n",
    "        section = (df.iloc[i].full_text_file + \"/\") * 2\n",
    "        pmcid = df.iloc[i].pmcid\n",
    "        filename = directory + section + \"pmc_json/\" + pmcid + \".xml.json\"\n",
    "        with open(filename) as paperjs:\n",
    "            jsfile = json.load(paperjs)\n",
    "\n",
    "        _id = df.iloc[i][\"cord_uid\"]\n",
    "        if \"title\" in jsfile.keys():\n",
    "            rows.append(dict(cord_uid=_id, section=\"title\", subsection=0, text=jsfile[\"title\"]))\n",
    "        else:\n",
    "            rows.append(dict(cord_uid=_id, section=\"title\", subsection=0, text=df.iloc[i].title))\n",
    "        if \"abstract\" in jsfile.keys():\n",
    "            if len(jsfile[\"abstract\"]) > 1:\n",
    "                for j in range(len(jsfile[\"abstract\"])):\n",
    "                    rows.append(dict(cord_uid=_id, section=\"abstract\", subsection=0, text=jsfile[\"abstract\"][j][\"text\"]))\n",
    "            else:\n",
    "                rows.append(dict(cord_uid=_id, section=\"abstract\", subsection=0, text=jsfile[\"abstract\"]))\n",
    "        elif \"abstract\" in jsfile[\"metadata\"].keys():\n",
    "            rows.append(dict(cord_uid=_id, section=\"abstract\", subsection=0, text=jsfile[\"metadata\"][\"abstract\"]))\n",
    "        else: \n",
    "            rows.append(dict(cord_uid=_id, section=\"abstract\", subsection=0, text=df.iloc[i].abstract))\n",
    "\n",
    "        sections = list(set([k[\"section\"] for k in jsfile[\"body_text\"]]))\n",
    "\n",
    "        for section in sections: \n",
    "            for l in range(len(jsfile[\"body_text\"])):\n",
    "                if jsfile[\"body_text\"][l][\"section\"] == section:\n",
    "                    if section == '':\n",
    "                        section = \"body_text\"\n",
    "                    rows.append(dict(cord_uid=_id, section=section, \n",
    "                                     subsection=l, text=jsfile[\"body_text\"][l][\"text\"]))\n",
    "\n",
    "        tables = extract_tables_from_json(jsfile)\n",
    "        for table in tables:\n",
    "            rows.append(dict(cord_uid=_id, section=table[0], subsection=table[1], text=table[2]))\n",
    "\n",
    "\n",
    "    for i in df[(df[\"has_pmc_xml_parse\"] == 0) & (df[\"has_pdf_parse\"] == 1)].index:\n",
    "        section = (df.iloc[i].full_text_file + \"/\") * 2\n",
    "        sha = df.iloc[i].sha\n",
    "        if len(sha.split(\"; \")) > 1:\n",
    "            sha = sha.split(\"; \")[0]\n",
    "        filename = directory + section + \"pdf_json/\" + sha + \".json\"\n",
    "        with open(filename) as paperjs:\n",
    "            jsfile = json.load(paperjs)\n",
    "\n",
    "        _id = df.iloc[i][\"cord_uid\"]\n",
    "        if \"title\" in jsfile.keys():\n",
    "            rows.append(dict(cord_uid=_id, section=\"title\", subsection=0, text=jsfile[\"title\"]))\n",
    "        else:\n",
    "            rows.append(dict(cord_uid=_id, section=\"title\", subsection=0, text=df.iloc[i].title))\n",
    "        if \"abstract\" in jsfile.keys():\n",
    "            if len(jsfile[\"abstract\"]) > 1:\n",
    "                for j in range(len(jsfile[\"abstract\"])):\n",
    "                    rows.append(dict(cord_uid=_id, section=\"abstract\", subsection=0, text=jsfile[\"abstract\"][j][\"text\"]))\n",
    "            else:\n",
    "                rows.append(dict(cord_uid=_id, section=\"abstract\", subsection=0, text=jsfile[\"abstract\"]))\n",
    "        elif \"abstract\" in jsfile[\"metadata\"].keys():\n",
    "            rows.append(dict(cord_uid=_id, section=\"abstract\", subsection=0, text=jsfile[\"metadata\"][\"abstract\"]))\n",
    "        else: \n",
    "            rows.append(dict(cord_uid=_id, section=\"abstract\", subsection=0, text=df.iloc[i].abstract))\n",
    "\n",
    "        sections = list(set([k[\"section\"] for k in jsfile[\"body_text\"]]))\n",
    "\n",
    "        for section in sections: \n",
    "            for l in range(len(jsfile[\"body_text\"])):\n",
    "                if jsfile[\"body_text\"][l][\"section\"] == section:\n",
    "                    if section == '':\n",
    "                        section = \"body_text\"\n",
    "                    rows.append(dict(cord_uid=_id, section=section, \n",
    "                                     subsection=l, text=jsfile[\"body_text\"][l][\"text\"]))\n",
    "\n",
    "        tables = extract_tables_from_json(jsfile)\n",
    "        for table in tables:\n",
    "            rows.append(dict(cord_uid=_id, section=table[0], subsection=table[1], text=table[2]))\n",
    "\n",
    "    for i in df[(df[\"has_pmc_xml_parse\"] == 0) & (df[\"has_pdf_parse\"] == 0)].index:\n",
    "        section = (df.iloc[i].full_text_file + \"/\") * 2\n",
    "        sha = df.iloc[i].sha\n",
    "\n",
    "        if len(sha.split(\"; \")) > 1:\n",
    "            sha = sha.split(\"; \")[0]\n",
    "        filename = directory + section + \"pdf_json/\" + sha + \".json\"\n",
    "\n",
    "        if len(sha) < 2: \n",
    "            bad_sha = True\n",
    "            try:\n",
    "                with open(directory + section + \"pmc_json/\" + df.iloc[i][\"pmcid\"] + \".xml.json\") as paperjs:\n",
    "                    jsfile = json.load(paperjs)\n",
    "            except:\n",
    "                pass\n",
    "        if bad_sha == True:\n",
    "            bad_sha = False\n",
    "            continue\n",
    "        with open(filename) as paperjs:\n",
    "            jsfile = json.load(paperjs)\n",
    "\n",
    "        _id = df.iloc[i][\"cord_uid\"]\n",
    "        if \"title\" in jsfile.keys():\n",
    "            rows.append(dict(cord_uid=_id, section=\"title\", subsection=0, text=jsfile[\"title\"]))\n",
    "        else:\n",
    "            rows.append(dict(cord_uid=_id, section=\"title\", subsection=0, text=df.iloc[i].title))\n",
    "        if \"abstract\" in jsfile.keys():\n",
    "            if len(jsfile[\"abstract\"]) > 1:\n",
    "                for j in range(len(jsfile[\"abstract\"])):\n",
    "                    rows.append(dict(cord_uid=_id, section=\"abstract\", subsection=0, text=jsfile[\"abstract\"][j][\"text\"]))\n",
    "            else:\n",
    "                rows.append(dict(cord_uid=_id, section=\"abstract\", subsection=0, text=jsfile[\"abstract\"]))\n",
    "        elif \"abstract\" in jsfile[\"metadata\"].keys():\n",
    "            rows.append(dict(cord_uid=_id, section=\"abstract\", subsection=0, text=jsfile[\"metadata\"][\"abstract\"]))\n",
    "        else: \n",
    "            rows.append(dict(cord_uid=_id, section=\"abstract\", subsection=0, text=df.iloc[i].abstract))\n",
    "\n",
    "        sections = list(set([k[\"section\"] for k in jsfile[\"body_text\"]]))\n",
    "\n",
    "        for section in sections: \n",
    "            for l in range(len(jsfile[\"body_text\"])):\n",
    "                if jsfile[\"body_text\"][l][\"section\"] == section:\n",
    "                    if section == '':\n",
    "                        section = \"body_text\"\n",
    "                    rows.append(dict(cord_uid=_id, section=section, \n",
    "                                     subsection=l, text=jsfile[\"body_text\"][l][\"text\"]))\n",
    "\n",
    "        tables = extract_tables_from_json(jsfile)\n",
    "        for table in tables:\n",
    "            rows.append(dict(cord_uid=_id, section=table[0], subsection=table[1], text=table[2]))\n",
    "\n",
    "    return(pd.DataFrame(rows))\n",
    "\n",
    "def parallelize_dataframe(df, func, n_cores=6):\n",
    "    df_split = np.array_split(df, 6)\n",
    "    pool = Pool(n_cores)\n",
    "    list(tqdm(pool.imap_unordered(func, df_split), total=len(df_split)))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "                    \n",
    "def init_list_cols():\n",
    "    return ['GGP', 'SO', 'TAXON', 'CHEBI', 'GO', 'CL', 'DNA', 'CELL_TYPE', 'CELL_LINE', 'RNA', 'PROTEIN', \n",
    "                          'DISEASE', 'CHEMICAL', 'CANCER', 'ORGAN', 'TISSUE', 'ORGANISM', 'CELL', 'AMINO_ACID',\n",
    "                          'GENE_OR_GENE_PRODUCT', 'SIMPLE_CHEMICAL', 'ANATOMICAL_SYSTEM', 'IMMATERIAL_ANATOMICAL_ENTITY',\n",
    "                          'MULTI-TISSUE_STRUCTURE', 'DEVELOPING_ANATOMICAL_STRUCTURE', 'ORGANISM_SUBDIVISION',\n",
    "                          'CELLULAR_COMPONENT', 'PATHOLOGICAL_FORMATION', \"lemma\", \"UMLS\"]\n",
    "        \n",
    "def pipeline(df):\n",
    "    languages = []\n",
    "    start_chars = []\n",
    "    end_chars = []\n",
    "    entities = []\n",
    "    sentences = []\n",
    "    lemmas = []\n",
    "    vectors = []\n",
    "    subsections = []\n",
    "    _ids = []\n",
    "    columns = []\n",
    "    nlp, linker = init_nlp()\n",
    "    nlps = init_ner()\n",
    "    \n",
    "    scispacy_ent_types = ['GGP', 'SO', 'TAXON', 'CHEBI', 'GO', 'CL', 'DNA', 'CELL_TYPE', 'CELL_LINE', 'RNA', 'PROTEIN', \n",
    "                          'DISEASE', 'CHEMICAL', 'CANCER', 'ORGAN', 'TISSUE', 'ORGANISM', 'CELL', 'AMINO_ACID',\n",
    "                          'GENE_OR_GENE_PRODUCT', 'SIMPLE_CHEMICAL', 'ANATOMICAL_SYSTEM', 'IMMATERIAL_ANATOMICAL_ENTITY',\n",
    "                          'MULTI-TISSUE_STRUCTURE', 'DEVELOPING_ANATOMICAL_STRUCTURE', 'ORGANISM_SUBDIVISION',\n",
    "                          'CELLULAR_COMPONENT', 'PATHOLOGICAL_FORMATION']\n",
    "    \n",
    "    for i in tqdm(range(len(df))):\n",
    "        doc = nlp(str(df.iloc[i][\"text\"]))\n",
    "        sents = [sent for sent in doc.sents]\n",
    "\n",
    "        if len(doc._.abbreviations) > 0 and doc._.language[\"language\"] == \"en\":\n",
    "            doc._.abbreviations.sort()\n",
    "            join_list = []\n",
    "            start = 0\n",
    "            for abbrev in doc._.abbreviations:\n",
    "                join_list.append(str(doc.text[start:abbrev.start_char]))\n",
    "                if len(abbrev._.long_form) > 5: #Increase length so \"a\" and \"an\" don't get un-abbreviated\n",
    "                    join_list.append(str(abbrev._.long_form))\n",
    "                else:\n",
    "                    join_list.append(str(doc.text[abbrev.start_char:abbrev.end_char]))\n",
    "                start = abbrev.end_char\n",
    "            # Reassign fixed body text to article in df.\n",
    "            new_text = \"\".join(join_list)\n",
    "            # We have new text. Re-nlp the doc for futher processing!\n",
    "            doc = nlp(new_text)\n",
    "\n",
    "        if doc._.language[\"language\"] == \"en\" and len(doc.text) > 5:\n",
    "            sents = [sent for sent in doc.sents]\n",
    "            for sent in sents:\n",
    "                languages.append(doc._.language[\"language\"])\n",
    "                sentences.append(sent.text)\n",
    "                vectors.append(sent.vector)\n",
    "                subsections.append(df.iloc[i][\"subsection\"])\n",
    "                lemmas.append([token.lemma_ for token in doc if not token.is_stop and re.search('[a-zA-Z]', token)])\n",
    "                doc_ents = []\n",
    "                for ent in sent.ents: \n",
    "                    if len(ent._.umls_ents) > 0:\n",
    "                        poss = linker.umls.cui_to_entity[ent._.umls_ents[0][0]].canonical_name\n",
    "                        doc_ents.append(poss)\n",
    "                entities.append(doc_ents)\n",
    "                _ids.append(df.iloc[i][\"cord_uid\"])\n",
    "                columns.append(df.iloc[i][\"section\"])\n",
    "        else:  \n",
    "            entities.append(\"[]\")\n",
    "            subsections.append(df.iloc[i][\"subsection\"])\n",
    "            sentences.append(doc.text)\n",
    "            vectors.append(np.zeros(200))\n",
    "            lemmas.append(\"[]\")\n",
    "            _ids.append(df.iloc[i,0])\n",
    "            languages.append(doc._.language[\"language\"])\n",
    "            columns.append(df.iloc[i][\"section\"])\n",
    "    \n",
    "    li1 = df[\"cord_uid\"].to_list()\n",
    "    li2 = df[\"section\"].to_list()\n",
    "    li3 = [str(i) for i in df.index]\n",
    "    sentence_id = [x + y + z for x,y,z in zip(li1,li2,li3)]\n",
    "\n",
    "    new_df = pd.DataFrame(data={\"cord_uid\": _ids, \"language\": languages,\n",
    "                                \"section\": columns, \"subsection\":subsections, \"sentence\": sentences,\n",
    "                                \"lemma\": lemmas, \"UMLS\": entities, \"w2vVector\": vectors})\n",
    "    \n",
    "    for col in scispacy_ent_types:\n",
    "        new_df[col] = \"[]\"\n",
    "    for j in tqdm(new_df.index):\n",
    "        if new_df.iloc[j][\"language\"] == \"en\":\n",
    "            for nlp in nlps:\n",
    "                doc = nlp(str(new_df.iloc[j][\"sentence\"]))\n",
    "                keys = list(set([ent.label_ for ent in doc.ents]))\n",
    "                for key in keys:\n",
    "\n",
    "                    # Some entity types are present in the model, but not in the documentation! \n",
    "                    # In that case, we'll just automatically add it to the df. \n",
    "                    if key not in scispacy_ent_types:\n",
    "                        new_df = pd.concat([new_df,pd.DataFrame(columns=[key])])\n",
    "                        new_df[key] = \"[]\"\n",
    "\n",
    "                    values = [ent.text for ent in doc.ents if ent.label_ == key]\n",
    "                    new_df.at[j,key] = values\n",
    "    if not os.path.exists(directory + \"df_parts/\"):\n",
    "        os.mkdir(directory + \"df_parts/\")\n",
    "        \n",
    "    new_df.drop(columns=[\"w2vVector\"]).to_pickle(directory + \"df_parts/\" + new_df.iloc[0][\"sentence_id\"] + \".ptext\", compression=\"gzip\")\n",
    "    new_df[[\"sentence_id\",\"w2vVector\"]].to_pickle(directory + \"df_parts/\" + new_df.iloc[0][\"sentence_id\"] + \".pvec\", compression=\"gzip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to where you have the metadata file. Make sure to untar/unzip all the folders.\n",
    "directory = \"/home/acorn/Documents/covid/CORD-19-research-challenge/\"\n",
    "\n",
    "# This method will parse the metadata, add all JSON information according to the highest\n",
    "# quality source available (Metadata > XML Parse > PDF Parse > leftovers in Metadata)\n",
    "# The function returns a pandas dataframe. \n",
    "\n",
    "df = process_metadata(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e820a4484e459b8c3dbd83277873f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acorn/anaconda3/envs/covid/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/acorn/anaconda3/envs/covid/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/acorn/anaconda3/envs/covid/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/acorn/anaconda3/envs/covid/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/acorn/anaconda3/envs/covid/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/acorn/anaconda3/envs/covid/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/acorn/anaconda3/envs/covid/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/acorn/anaconda3/envs/covid/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/acorn/anaconda3/envs/covid/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/acorn/anaconda3/envs/covid/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/acorn/anaconda3/envs/covid/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/acorn/anaconda3/envs/covid/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e5eb3646f4344a19ea1b193151f5586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1525.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d34f1da542d643b9b9d9cf5971d018d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1525.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0dcf575c0a94cd8ab85b594b3fb675e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1524.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34d98164b37e40e0a702067e0fa4784f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1525.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a89d74e9e25b4a329f4623d56087070d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1525.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "777d704564bf4e819af0f8c856886e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1525.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc2eca16f74243379668f181f8d48477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5788.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f177db8b01945b4a8c89ea86c8992b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5728.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ff3447beba4a0cb4b7e8dafda46316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6285.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1571d0d5c61e4c2285a0449a5284837d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6403.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "441156dc00ad493d95943b4a767f6c2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6615.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-5:\n",
      "Process ForkPoolWorker-7:\n",
      "Process ForkPoolWorker-6:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/queues.py\", line 352, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/covid/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7461d4141909>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparallelize_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_cores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-8108ddfdda02>\u001b[0m in \u001b[0;36mparallelize_dataframe\u001b[0;34m(df, func, n_cores)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdf_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_cores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/covid/lib/python3.7/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/covid/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1106\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1108\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1109\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/covid/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    735\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m                     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/covid/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/queues.py\", line 352, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/acorn/anaconda3/envs/covid/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "parallelize_dataframe(df, pipeline, n_cores=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to whatever version of dataset we're on at this point\n",
    "version = \"v7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate and save processed text. \n",
    "df = pd.concat([pd.read_csv(directory + \"df_parts/\" + f) for f in os.listdir(directory) if f.endswith(\".ptext\")])\n",
    "df.to_pickle(directory + version + \"processedText.pkl\", compression=\"gzip\")\n",
    "del df\n",
    "\n",
    "# Concatenate and save processed vectors. \n",
    "df = pd.concat([pd.read_csv(directory + \"df_parts/\" + f) for f in os.listdir(directory) if f.endswith(\".pvec\")])\n",
    "df.to_pickle(directory + version + \"processedVecs.pkl\", compression=\"gzip\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup\n",
    "\n",
    "The following code is simply cleanup after the extraction process. First, we'll save the text data in a json file. Next we can save the vector data because it's large, and most people won't be using it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "for col in columns[8:]: \n",
    "    if col != \"sentence_id\":\n",
    "        df[col] = [ast.literal_eval(i) for i in df[col].to_list()]\n",
    "vectors = df[\"w2vVector\"]\n",
    "df.drop(columns=[\"w2vVector\"], inplace=True)\n",
    "df.to_json(\"v7_text.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [i for i in vectors.apply(lambda x: \n",
    "                       np.fromstring(\n",
    "                           x.replace('\\n','')\n",
    "                            .replace('[','')\n",
    "                            .replace(']','')\n",
    "                            .replace('  ',' '), sep=' '))]\n",
    "vec_df = pd.DataFrame.from_records(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acorn/anaconda3/envs/covid/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/acorn/anaconda3/envs/covid/lib/python3.7/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "translator = Translator()\n",
    "nlp = init_nlp()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c85f52f9ca84f43bacaa0178e25075c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=34922.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "main = df[df[\"section\"] == \"title\"][[\"text\", \"cord_uid\"]]\n",
    "main = main.groupby(by=[\"cord_uid\"]).sum()\n",
    "main.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "languages = []\n",
    "for i in tqdm(main.index): \n",
    "    languages.append(nlp(str(main.iloc[i][\"text\"]))._.language[\"language\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "main[\"language\"] = languages\n",
    "foreign = main[main[\"language\"] != \"en\"][\"cord_uid\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1815"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(foreign)\n",
    "#translator.translate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00ajdmac</td>\n",
       "      <td>Chapter 47 Felidae</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00nzuutv</td>\n",
       "      <td>Gastroenteritis aguda</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>00rugcs8</td>\n",
       "      <td>Caracter√≠sticas epidemiol√≥gicas, cl√≠nicas y te...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>01rpxu69</td>\n",
       "      <td>SARS hCoV papain-like protease is a unique Lys...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>022skl85</td>\n",
       "      <td>Infecciones por el virus de Epstein-Barr y cit...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34816</th>\n",
       "      <td>zvwy7bgt</td>\n",
       "      <td>Convergencia mundial de las enfermedades infec...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34818</th>\n",
       "      <td>zw03w767</td>\n",
       "      <td>0</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34821</th>\n",
       "      <td>zw5kt090</td>\n",
       "      <td>Modeling site-specific amino-acid preferences ...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34847</th>\n",
       "      <td>zx02gxw0</td>\n",
       "      <td>MERS-CoV spike protein: a key target for antiv...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34882</th>\n",
       "      <td>zyadh6x8</td>\n",
       "      <td>Neuroinfection &amp; neuroimmunology: New opportun...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1815 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cord_uid                                               text language\n",
       "5      00ajdmac                                 Chapter 47 Felidae       nl\n",
       "17     00nzuutv                              Gastroenteritis aguda       nl\n",
       "20     00rugcs8  Caracter√≠sticas epidemiol√≥gicas, cl√≠nicas y te...       es\n",
       "51     01rpxu69  SARS hCoV papain-like protease is a unique Lys...       fr\n",
       "58     022skl85  Infecciones por el virus de Epstein-Barr y cit...       es\n",
       "...         ...                                                ...      ...\n",
       "34816  zvwy7bgt  Convergencia mundial de las enfermedades infec...       es\n",
       "34818  zw03w767                                                  0  UNKNOWN\n",
       "34821  zw5kt090  Modeling site-specific amino-acid preferences ...       es\n",
       "34847  zx02gxw0  MERS-CoV spike protein: a key target for antiv...       no\n",
       "34882  zyadh6x8  Neuroinfection & neuroimmunology: New opportun...       fr\n",
       "\n",
       "[1815 rows x 3 columns]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main[main[\"language\"] != \"en\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
