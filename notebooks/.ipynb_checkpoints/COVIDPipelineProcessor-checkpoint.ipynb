{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is known about transmission, incubation, and environmental stability\n",
    "## COVID-19 Open Research Dataset Challenge (CORD-19)\n",
    "\n",
    "### Task Details\n",
    "What is known about transmission, incubation, and environmental stability? What do we know about natural history, transmission, and diagnostics for the virus? What have we learned about infection prevention and control?\n",
    "\n",
    "The first question we need to ask is what we mean by transmission, incubation, and environmental stability -- or, rather, what should a computer understand when we ask this? We can go about encoding this information in several ways: 1) keywords for analysis in some kind of TF-IDF format, probably including a list of synonyms that we would need to develop by hand, 2) high-dimensional vectors vis a vis word2vec or GloVe, or 3) using heavy, but state-of-the-art transformer models for vectorization. \n",
    "\n",
    "Keywords probably aren't going to give us the robust results we're looking for, because typical pre-processing methods remove all sorts of punctuation and numbers, but these are really important in biomedical texts! We could skip the pre-processing except for removing stop words, but we'd still need to address the fact that keywords have synonyms, and we'd need to hand-write these. But there may be an easier way to get better results without all the hassle. \n",
    "\n",
    "I propose method 2: spaCy is a popular NLP package that's blazingly fast and has (mostly) everything we need to process the text. It'll break sentences apart, lemmatize, and even provide vectors for us. Spacy vectors are somewhat simplistic because the vector of several tokens is just the average of the vectors of each token individually -- so we may not get state of the art results. But we'll get them fast, and we'll know if we need to change something up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Getting the data ready\n",
    "The data is formatted in many files as json; let's put it into something easier to work with for NLP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall spacy -y\n",
    "#!pip install spacy[cuda100] scispacy spacy_langdetect scipy https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.3/en_core_sci_lg-0.2.3.tar.gz\n",
    "#!pip install -U spacy[cuda100]\n",
    "#!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.3/en_core_sci_lg-0.2.3.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "import numpy as np\n",
    "import scispacy\n",
    "import json\n",
    "import spacy\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.spatial import distance\n",
    "import ipywidgets as widgets\n",
    "from scispacy.abbreviation import AbbreviationDetector\n",
    "from spacy_langdetect import LanguageDetector\n",
    "# UMLS linking will find concepts in the text, and link them to UMLS. \n",
    "from scispacy.umls_linking import UmlsEntityLinker\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time for NLP!\n",
    "\n",
    "Let's load our language model. Based on the type of text we'll be dealing with, we want something that's been pretrained on biomedical texts, as the vocabulary and statistical distribution of words is much different from, say, the news or Wikipedia articles. Luckily, there's already pre-trained models for spacy, so let's load the largest one we can! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juanluis/covid19/env/lib/python3.6/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/home/juanluis/covid19/env/lib/python3.6/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#nlp = spacy.load(\"en_core_sci_lg\")\n",
    "nlp = spacy.load(\"en_core_sci_lg\", disable=[\"tagger\"])\n",
    "\n",
    "# We also need to detect language, or else we'll be parsing non-english text \n",
    "# as if it were English. \n",
    "nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\n",
    "\n",
    "# Add the abbreviation pipe to the spacy pipeline. Only need to run this once.\n",
    "abbreviation_pipe = AbbreviationDetector(nlp)\n",
    "nlp.add_pipe(abbreviation_pipe)\n",
    "\n",
    "# Our linker will look up named entities/concepts in the UMLS graph and normalize\n",
    "# the data for us. \n",
    "linker = UmlsEntityLinker(resolve_abbreviations=True)\n",
    "nlp.add_pipe(linker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a vector for COVID-19\n",
    "\n",
    "One last thing. COVID-19 is a new word, and doesn't exist in the vocabulary for our spaCy model. We'll need to add it manually; let's try setting it to equal the average vector of words that should represent what COVID-19 refers to, and see if that works. I'm not an expert so I just took definitions from Wikipedia and the etiology section of https://onlinelibrary.wiley.com/doi/full/10.1002/jmv.25740. There's a much better way of doing this (fine-tuning the model on our corpus) but I have no idea how to do this in spaCy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.vocab import Vocab\n",
    "new_vector = nlp(\n",
    "               \"\"\"Single‐stranded RNA virus, belongs to subgenus \n",
    "                   Sarbecovirus of the genus Betacoronavirus.5 Particles \n",
    "                   contain spike and envelope, virions are spherical, oval, or pleomorphic \n",
    "                   with diameters of approximately 60 to 140 nm.\n",
    "                   Also known as severe acute respiratory syndrome coronavirus 2, \n",
    "                   previously known by the provisional name 2019 novel coronavirus \n",
    "                   (2019-nCoV), is a positive-sense single-stranded RNA virus. It is \n",
    "                   contagious in humans and is the cause of the ongoing pandemic of \n",
    "                   coronavirus disease 2019 that has been designated a \n",
    "                   Public Health Emergency of International Concern\"\"\").vector\n",
    "\n",
    "vector_data = {\"COVID-19\": new_vector,\n",
    "               \"2019-nCoV\": new_vector,\n",
    "               \"SARS-CoV-2\": new_vector}\n",
    "\n",
    "vocab = Vocab()\n",
    "for word, vector in vector_data.items():\n",
    "    nlp.vocab.set_vector(word, vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check\n",
    "Alright, let's check if this work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5324297344523997 \n",
      " 0.34796126970622626 \n",
      " 0.7016356811120861\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    nlp(\"COVID-19\").similarity(nlp(\"novel coronavirus\")), \"\\n\",\n",
    "    nlp(\"SARS-CoV-2\").similarity(nlp(\"severe acute respiratory syndrome\")), \"\\n\",\n",
    "    nlp(\"COVID-19\").similarity(nlp(\"sickness caused by a new virus\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I guess we'll find out if that's good enough for our purposes! Let's save it so other people can use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp.to_disk('/home/acorn/Documents/covid-19-en_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the texts is particularly long, so we need to increase the max_length attribute of nlp to more then 1.25mil. The alternative would be cutting the length of the article or dropping it entirely (I believe there's some sort of anomaly with this particular article), but we'll keep it for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.max_length=2000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to replace all abbreviations with their long forms. This is important for semantic indexing because the model has probably seen words like \"Multiple sclerosis\" but may have seen the abbreviation \"MS\" in different contexts. That means their vector representations are different, and we don't want that! \n",
    "\n",
    "So here we'll add the abbreviation expansion module to our scispaCy pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbreviation \t Definition\n",
      "PACS \t (1064, 1068) picture archiving and communication system\n",
      "FOV \t (2867, 2870) field of view\n",
      "GGO \t (4432, 4435) ground glass opacity\n",
      "Detect COVID-19 \t (5459, 5474) Detect COVID-19\n",
      "Detect COVID-19 \t (8757, 8772) Detect COVID-19\n",
      "Detect COVID-19 \t (5508, 5523) Detect COVID-19\n",
      "Detect COVID-19 \t (9108, 9123) Detect COVID-19\n",
      "Detect COVID-19 \t (16052, 16067) Detect COVID-19\n",
      "Detect COVID-19 \t (5751, 5766) Detect COVID-19\n",
      "Detect COVID-19 \t (5595, 5610) Detect COVID-19\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(df.iloc[0].text)\n",
    "\n",
    "print(\"Abbreviation\", \"\\t\", \"Definition\")\n",
    "for abrv in doc._.abbreviations[0:10]:\n",
    "\tprint(f\"{abrv} \\t ({abrv.start_char}, {abrv.end_char}) {abrv._.long_form}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we get some weird results towards the end if you print **all** of them (lots of a's being converted to at's, but we can ignore that for now. If we need to remove stop words later, we can. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Vector DataFrames\n",
    "Appending to a dataframe increases time to copy data linearly because df.append copies the entire object. The following will take an article's text, break it into sentences, and vectorize each sentence (using scispacy's pre-trained word2vec model). Finally, the list of dicts is loaded as a DataFrame and saved.\n",
    "\n",
    "So here's the real meat of our pre-processing. This is really heavy because it processes line-by-line and then generates a lot of metadata (entities, vectors). We can break it into pieces later depending on the task we want to use this information for, but querying lines is a lot more useful that querying whole documents when you want to know about something specific like seroconversion, spike proteins, or something else. Once you identify lines of interest, you can generate more data about the actual document, since each line will be indexed with document, start and end character, entities, vectors, and language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_cleaner(df):\n",
    "    df.fillna(\"Empty\", inplace=True) # If we leave floats (NaN), spaCy will break.\n",
    "    for i in df.index:\n",
    "        for j in range(len(df.columns)):\n",
    "            if \" q q\" in df.iloc[i,j]:\n",
    "                df.iloc[i,j] = df.iloc[i,j].replace(\" q q\",\"\") # Some articles are filled with \" q q q q q q q q q\"\n",
    "                \n",
    "def pipeline(df, column, dataType, filename):\n",
    "    create = pd.DataFrame(columns={\"_id\",\"language\",\"section\",\"sentence\",\"startChar\",\"endChar\",\"entities\",\"lemma\",\"w2vVector\"})\n",
    "    create.to_csv(filename + \".csv\", index=False)\n",
    "    \n",
    "    docs = nlp.pipe(df[column].astype(str))\n",
    "    i = -1\n",
    "    for doc in tqdm(docs):\n",
    "        languages = []\n",
    "        start_chars = []\n",
    "        end_chars = []\n",
    "        entities = []\n",
    "        sentences = []\n",
    "        #vectors = []\n",
    "        _ids = []\n",
    "        columns = []\n",
    "        lemmas = []\n",
    "        i = i + 1\n",
    "        if len(doc._.abbreviations) > 0 and doc._.language[\"language\"] == \"en\":\n",
    "            doc._.abbreviations.sort()\n",
    "            join_list = []\n",
    "            start = 0\n",
    "            for abbrev in doc._.abbreviations:\n",
    "                join_list.append(str(doc.text[start:abbrev.start_char]))\n",
    "                if len(abbrev._.long_form) > 5: #Increase length so \"a\" and \"an\" don't get un-abbreviated\n",
    "                    join_list.append(str(abbrev._.long_form))\n",
    "                else:\n",
    "                    join_list.append(str(doc.text[abbrev.start_char:abbrev.end_char]))\n",
    "                start = abbrev.end_char\n",
    "            # Reassign fixed body text to article in df.\n",
    "            new_text = \"\".join(join_list)\n",
    "            # We have new text. Re-nlp the doc for futher processing!\n",
    "            doc = nlp(new_text)\n",
    "        \n",
    "        sents = [sent for sent in doc.sents]\n",
    "        if doc._.language[\"language\"] == \"en\" and len(doc.text) > 5:\n",
    "            for sent in sents:\n",
    "                languages.append(doc._.language[\"language\"])\n",
    "                sentences.append(sent.text)\n",
    "                vectors.append(sent.vector)\n",
    "                start_chars.append(sent.start_char)\n",
    "                end_chars.append(sent.end_char)\n",
    "                doc_ents = []\n",
    "                for ent in sent.ents: \n",
    "                    if len(ent._.umls_ents) > 0:\n",
    "                        poss = linker.umls.cui_to_entity[ent._.umls_ents[0][0]].canonical_name\n",
    "                        doc_ents.append(poss)\n",
    "                entities.append(doc_ents)\n",
    "                _ids.append(df.iloc[i,0])\n",
    "                if dataType == \"tables\":\n",
    "                    columns.append(df.iloc[i][\"figure\"])\n",
    "                elif dataType == \"text\":\n",
    "                    columns.append(column)\n",
    "                lemmatized_doc = \" \".join([token.lemma_ for token in doc])\n",
    "                lemmas.append(lemmatized_doc)\n",
    "        else: \n",
    "            start_chars.append(0)\n",
    "            end_chars.append(len(doc.text))\n",
    "            entities.append(\"Non-English\")\n",
    "            sentences.append(doc.text)\n",
    "            vectors.append(np.zeros(200))\n",
    "            _ids.append(df.iloc[i,0])\n",
    "            languages.append(doc._.language[\"language\"])\n",
    "            if dataType == \"tables\":\n",
    "                columns.append(df.iloc[i][\"figure\"])\n",
    "            elif dataType == \"text\":\n",
    "                columns.append(column)\n",
    "            lemmas.append(\"Non-English\")\n",
    "            \n",
    "        rows = pd.DataFrame(data={\"_id\": _ids, \"language\": languages, \"section\": columns, \"sentence\": sentences, \n",
    "            \"startChar\": start_chars, \"endChar\": end_chars, \"entities\": entities, \"lemma\": lemmas, \"w2vVector\": vectors})\n",
    "        rows.to_csv(filename + '.csv', mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ede4d6bdd25a415ebbe0f9f6970ea64e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=146.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27cda11d977849ba985640ca1ae5ab67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.89 GiB for an array with shape (1019544, 8, 64, 2) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-7b8779d59aa8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./unnabreviated_parts/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataType\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-945fdb73975e>\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(df, column, dataType, filename)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mlanguages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mstart_chars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/covid19/env/lib/python3.6/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/covid19/env/lib/python3.6/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1106\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1108\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1109\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/covid19/env/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mpipe\u001b[0;34m(self, texts, as_tuples, n_threads, batch_size, disable, cleanup, component_cfg, n_process)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0moriginal_strings_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mnr_seen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/covid19/env/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m_pipe\u001b[0;34m(docs, proc, kwargs)\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1112\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/covid19/env/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m_pipe\u001b[0;34m(docs, proc, kwargs)\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1112\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/covid19/env/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m_pipe\u001b[0;34m(docs, proc, kwargs)\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1112\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mpipe\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/covid19/env/lib/python3.6/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mminibatch\u001b[0;34m(items, size)\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mpipe\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.predict\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.greedy_parse\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/covid19/env/lib/python3.6/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/covid19/env/lib/python3.6/site-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.ParserModel.begin_update\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.ParserStepModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.precompute_hiddens.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/covid19/env/lib/python3.6/site-packages/spacy/_ml.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X, drop)\u001b[0m\n\u001b[1;32m    180\u001b[0m         )\n\u001b[1;32m    181\u001b[0m         \u001b[0mYf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mYf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdY_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/covid19/env/lib/python3.6/site-packages/spacy/_ml.py\u001b[0m in \u001b[0;36m_add_padding\u001b[0;34m(self, Yf)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mYf_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mYf_padded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/covid19/env/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 3.89 GiB for an array with shape (1019544, 8, 64, 2) and data type float32"
     ]
    }
   ],
   "source": [
    "columns_to_process = [\"text\"]\n",
    "files = [f for f in os.listdir(\"./unnabreviated_parts/\") if f.startswith(\"unna\") and not f.endswith(\"csv\")]\n",
    "for f in tqdm(files):\n",
    "    f = \"./unnabreviated_parts/\" + f\n",
    "    df = pd.read_csv(f)\n",
    "    pipeline(df=df, column=\"text\", dataType=\"text\", filename=f)\n",
    "    del df\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatized Text\n",
    "\n",
    "Just in case we need it, let's do some text cleaning and include that in a different column. Lemmatization normalizes data so that when you're creating word clouds or simplified TF-IDF, the number of dimesions you're dealing with are significantly reduced. It's also nice to remove words that don't contribute much meaning, but do note that removing stop-words will make neural models less accurate depending on the task you're using them for.\n",
    "\n",
    "#### This is included in the pipeline() function above, but expanded here if you only need this functionality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_my_text(df, column):\n",
    "    lemma_column = []\n",
    "    for i in df.index:\n",
    "        if df.iloc[i][\"language\"] == \"en\":\n",
    "            doc = nlp(str(df.iloc[i][column]), disable=[\"ner\",\"linker\", \"language_detector\"])\n",
    "            lemmatized_doc = \" \".join([token.lemma_ for token in doc])\n",
    "            lemma_column.append(lemmatized_doc)\n",
    "        else: \n",
    "            lemma_column.append(\"Non-English\")\n",
    "    return lemma_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>citations</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0e38333bff68345492526fd39b70d1b18969cb83</td>\n",
       "      <td>['Clinical features of patients infected with ...</td>\n",
       "      <td>Deep Learning-based Detection for COVID-19 fro...</td>\n",
       "      <td>Accurate and rapid diagnosis of COVID-19 suspe...</td>\n",
       "      <td>huge amount of efforts for radiologists, which...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4ce668fe6eee9f59ed5ad0dfc0e9787777acd3be</td>\n",
       "      <td>['Effect of exacerbation on quality of life in...</td>\n",
       "      <td>Anti-microbial immunity is impaired in COPD pa...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Chronic obstructive pulmonary disease (COPD) i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12920263b2846c61d6f2b6105189367a8ff1bc39</td>\n",
       "      <td>['Genetic improvement of litter size in pigs',...</td>\n",
       "      <td>A diallel of the mouse Collaborative Cross fou...</td>\n",
       "      <td>Reproductive success in the eight founder stra...</td>\n",
       "      <td>A critical part of fitness is optimizing the b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>af266fac8970a7960e96630a67d91bec5dda0335</td>\n",
       "      <td>['Handbook of infectious disease data analysis...</td>\n",
       "      <td>Estimating the generation interval for COVID-1...</td>\n",
       "      <td>Estimating key infectious disease parameters f...</td>\n",
       "      <td>In order to plan intervention strategies aimed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57a86b3acd182c955877afd4792719a4e9a0ac32</td>\n",
       "      <td>['A novel coronavirus outbreak of global healt...</td>\n",
       "      <td>Cross-sectional Study Affiliations</td>\n",
       "      <td>Background: So far, the psychological impact o...</td>\n",
       "      <td>Since December 2019, coronavirus disease 2019 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>ba3efcd6b74e55327fd7db470d824fc18943f30e</td>\n",
       "      <td>['Potential for global spread of a novel coron...</td>\n",
       "      <td>Evaluating the impact of international airline...</td>\n",
       "      <td>Global airline networks play a key role in the...</td>\n",
       "      <td>results indicate multiple countries (many with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>5daceb492bad28a61fc43c4be83a164042d8c430</td>\n",
       "      <td>['∆3 cells, as indicated. The experiment was 8...</td>\n",
       "      <td>Recombinant rotaviruses rescued by reverse gen...</td>\n",
       "      <td>Rotavirus (RV</td>\n",
       "      <td>To rescue recombinant RV strain SA11 (rRV-WT),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1dd898b5ca1ae70ec0e3cad89fc87a165002a99e</td>\n",
       "      <td>['Pathology of US porcine epidemic diarrhea vi...</td>\n",
       "      <td>Using heterogeneity in the population structur...</td>\n",
       "      <td>In 2013, U.S. swine producers were confronted ...</td>\n",
       "      <td>The 2013 emergence of porcine epidemic diarrho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>1aa3e788fc6b03c147e37200c3b011ca7a289a5c</td>\n",
       "      <td>['Mechanism of nucleic acid unwinding by SARS-...</td>\n",
       "      <td>Dark Proteome of Newly Emerged SARS-CoV-2 in C...</td>\n",
       "      <td>Recently emerged Wuhan's novel coronavirus des...</td>\n",
       "      <td>World health organization third of genome code...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>9da357734a3bfc5b51dc2c63b6b4ab28930767ab</td>\n",
       "      <td>['Immune responses in COVID-19 and potential v...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>well as cytokines important for immune reactio...</td>\n",
       "      <td>Coronavirus disease 2019 (COVID-19) is a respi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>201 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          _id  \\\n",
       "0    0e38333bff68345492526fd39b70d1b18969cb83   \n",
       "1    4ce668fe6eee9f59ed5ad0dfc0e9787777acd3be   \n",
       "2    12920263b2846c61d6f2b6105189367a8ff1bc39   \n",
       "3    af266fac8970a7960e96630a67d91bec5dda0335   \n",
       "4    57a86b3acd182c955877afd4792719a4e9a0ac32   \n",
       "..                                        ...   \n",
       "196  ba3efcd6b74e55327fd7db470d824fc18943f30e   \n",
       "197  5daceb492bad28a61fc43c4be83a164042d8c430   \n",
       "198  1dd898b5ca1ae70ec0e3cad89fc87a165002a99e   \n",
       "199  1aa3e788fc6b03c147e37200c3b011ca7a289a5c   \n",
       "200  9da357734a3bfc5b51dc2c63b6b4ab28930767ab   \n",
       "\n",
       "                                             citations  \\\n",
       "0    ['Clinical features of patients infected with ...   \n",
       "1    ['Effect of exacerbation on quality of life in...   \n",
       "2    ['Genetic improvement of litter size in pigs',...   \n",
       "3    ['Handbook of infectious disease data analysis...   \n",
       "4    ['A novel coronavirus outbreak of global healt...   \n",
       "..                                                 ...   \n",
       "196  ['Potential for global spread of a novel coron...   \n",
       "197  ['∆3 cells, as indicated. The experiment was 8...   \n",
       "198  ['Pathology of US porcine epidemic diarrhea vi...   \n",
       "199  ['Mechanism of nucleic acid unwinding by SARS-...   \n",
       "200  ['Immune responses in COVID-19 and potential v...   \n",
       "\n",
       "                                                 title  \\\n",
       "0    Deep Learning-based Detection for COVID-19 fro...   \n",
       "1    Anti-microbial immunity is impaired in COPD pa...   \n",
       "2    A diallel of the mouse Collaborative Cross fou...   \n",
       "3    Estimating the generation interval for COVID-1...   \n",
       "4                   Cross-sectional Study Affiliations   \n",
       "..                                                 ...   \n",
       "196  Evaluating the impact of international airline...   \n",
       "197  Recombinant rotaviruses rescued by reverse gen...   \n",
       "198  Using heterogeneity in the population structur...   \n",
       "199  Dark Proteome of Newly Emerged SARS-CoV-2 in C...   \n",
       "200                                                NaN   \n",
       "\n",
       "                                              abstract  \\\n",
       "0    Accurate and rapid diagnosis of COVID-19 suspe...   \n",
       "1                                                   []   \n",
       "2    Reproductive success in the eight founder stra...   \n",
       "3    Estimating key infectious disease parameters f...   \n",
       "4    Background: So far, the psychological impact o...   \n",
       "..                                                 ...   \n",
       "196  Global airline networks play a key role in the...   \n",
       "197                                      Rotavirus (RV   \n",
       "198  In 2013, U.S. swine producers were confronted ...   \n",
       "199  Recently emerged Wuhan's novel coronavirus des...   \n",
       "200  well as cytokines important for immune reactio...   \n",
       "\n",
       "                                                  text  \n",
       "0    huge amount of efforts for radiologists, which...  \n",
       "1    Chronic obstructive pulmonary disease (COPD) i...  \n",
       "2    A critical part of fitness is optimizing the b...  \n",
       "3    In order to plan intervention strategies aimed...  \n",
       "4    Since December 2019, coronavirus disease 2019 ...  \n",
       "..                                                 ...  \n",
       "196  results indicate multiple countries (many with...  \n",
       "197  To rescue recombinant RV strain SA11 (rRV-WT),...  \n",
       "198  The 2013 emergence of porcine epidemic diarrho...  \n",
       "199  World health organization third of genome code...  \n",
       "200  Coronavirus disease 2019 (COVID-19) is a respi...  \n",
       "\n",
       "[201 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "df = pd.concat([i for i in [pd.read_csv(f) for f in files]])\n",
    "timestamp = time.strftime(\"%Y%m%d\")\n",
    "df.to_csv(f\"covid_TitleAbstract_processed-{timestamp}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and do the same for the tables; we want to be able to search them too! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asking the right questions\n",
    "\n",
    "Our model isn't trained to answer questions. It's trained to represent words based on their statistical distribution in relation to other words -- one way of measuring semantic similarity, or closeness in meaning. Instead of feeding our model questions, we should be making statements and trying to find statements that are semantically similar within the corpus. \n",
    "\n",
    "So, let's do a bit of editing on the original questions asked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = \"\"\"Range of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery.\n",
    "Prevalence of asymptomatic shedding and transmission. \n",
    "Prevalence of asymptomatic shedding and transmission in children, infants, and young people.\n",
    "Seasonality of transmission of the virus. Times when the virus is transmitted, i.e. winter, summer, autumn, spring, during cold weather, or in different climates.\n",
    "Physical science of the coronavirus (e.g., charge distribution, adhesion to hydrophilic or hydrophobic surfaces, environmental survival to inform decontamination efforts for affected areas and provide information about viral shedding).\n",
    "Persistence and stability on a multitude of substrates and sources (nasal discharge, sputum, urine, fecal matter, blood, bodily fluids and secretions).\n",
    "Persistence of virus on surfaces of different materials (copper, stainless steel, plastic).\n",
    "Natural history of the virus and shedding of it from an infected person.\n",
    "Implementation of diagnostics and products to improve clinical processes.\n",
    "Disease models, including animal models for infection, disease and transmission.\n",
    "Tools and studies to monitor phenotypic change and potential adaptation of the virus.\n",
    "Immune response and immunity to the virus.\n",
    "Effectiveness of movement control strategies to prevent secondary transmission in health care and community settings.\n",
    "Effectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings.\n",
    "Role of the environment in transmission.\"\"\"\n",
    "queries = queries.splitlines()\n",
    "queries_df = pd.DataFrame(data=[{\"query\":query} for query in queries])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will vectorize each of our query sentences, and store those vectors in a DataFrame for us. That way, we have a numeric representation of the meaning of our queries, and they're conveniently indexed by number. Later, we can use these index numbers for cross-reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector_list = []\n",
    "for i in tqdm(range(len(queries))):\n",
    "    doc = nlp(queries[i])\n",
    "    vec = doc.vector\n",
    "    query_vector_list.append({\"_id\": f\"query_{i}\", \"vector\": vec})\n",
    "    \n",
    "query_vector_df = pd.DataFrame(data=query_vector_list)\n",
    "#query_vector_df.to_csv(f\"query_vecs-{time.strftime(\"%Y%m%d\")}.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we measure similarity? \n",
    "\n",
    "We can calculate the vector cosine distances between all queries and sentences in the corpus. Cosine distance is a pretty typical measurement in NLP for the similarity between two vectors -- however, there are other measurements (Euclidean distance takes magnitude into consideration; cosine similarity only cares about the angle between pair-wise values on an axis). We'll go with cosine similarity for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = distance.cdist([value for value in query_vector_df[\"vector\"]], [value for value in vector_df[\"vector\"].values], \"cosine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save this as a searchable df. We'll keep \"vector_df\" on the backburner for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_searchable_df = vector_df.drop(columns=[\"vector\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column with cosine distances for each query vs the sentence\n",
    "for i in range(len(queries)):\n",
    "    w2v_searchable_df[f\"query_{i}_distance\"] = distances[i]\n",
    "#w2v_searchable_df.to_csv(f\"covid_w2v_searchable-{time.strftime(\"%Y%m%d\")}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Getting Results\n",
    "\n",
    "Now we can start to use our cosine similarity measurements! It's essentially like a search engine, but without the need for keywords, as long as you have a good idea what you're looking for. \n",
    "\n",
    "\n",
    "\n",
    "#### Run this cell below if you've already got the data from Part 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v_searchable_df = pd.read_csv(\"covid_w2v_searchable.csv\")\n",
    "df = pd.read_csv(\"covid_data_full-03212020.csv\")\n",
    "#queries_df = pd.read_csv(\"queries.csv\")\n",
    "#vector_df = pd.read_csv(\"covid_vectors.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop(columns=[\"entities\"],inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out our top results, and see if this was useful! Here we pass over each column, sorting for the highest values (most similar), and printing the top 20 hits. Are the results what we expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(queries)):\n",
    "    columnName = f\"query_{i}_distance\"\n",
    "    context = w2v_searchable_df.sort_values(by=columnName)[[\"_id\",\"start_span\",\"end_span\"]][:20]\n",
    "    ix = context[\"_id\"].to_list()\n",
    "    spans1 = context[\"start_span\"].to_list()\n",
    "    spans2 = context[\"end_span\"].to_list()\n",
    "    print(queries[i] + \"\\n\")\n",
    "    for j in range(len(context.index)):\n",
    "        print(f\"Rank {j+1}: \" + str(df[df[\"_id\"] == ix[j]].iloc[0][\"text\"])[spans1[j]:spans2[j]] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, this doesn't look half bad! \n",
    "\n",
    "Since I'm just a humble linguist and I don't know much about virology, I can look at the above and learn a great deal in a short amount of time. I assume a biologist or expert in the field would have a better idea of what to look for. My code isn't super useful for them, so... \n",
    "\n",
    "Alright, let's turn it into a function so we don't have all this free-flying code all over the place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_these(queries, n):\n",
    "    \n",
    "    # queries : a list of sentences, preferably statements as described in Part 1. \n",
    "    #           The goal is to make these statements similar to something you're looking for.\n",
    "    # n : The number of top hits you'd like to print out per query.\n",
    "    query_vector_list = []\n",
    "    for i in tqdm(range(len(queries))):\n",
    "        doc = nlp(queries[i])\n",
    "        vec = doc.vector\n",
    "        query_vector_list.append({\"_id\": f\"query_{i}\", \"vector\": vec})\n",
    "    \n",
    "    \n",
    "    query_vector_df = pd.DataFrame(data=query_vector_list)\n",
    "    \n",
    "    distances = distance.cdist([value for value in query_vector_df[\"vector\"]], [value for value in vector_df[\"vector\"].values], \"cosine\")\n",
    "    \n",
    "    for i in range(len(queries)):\n",
    "        w2v_searchable_df[f\"temp_query_{i}_distance\"] = distances[i]\n",
    "    \n",
    "    for i in range(len(queries)):\n",
    "        columnName = f\"temp_query_{i}_distance\"\n",
    "        context = w2v_searchable_df.sort_values(by=columnName)[[\"_id\",\"start_span\",\"end_span\"]][:n]\n",
    "        ix = context[\"_id\"].to_list()\n",
    "        spans1 = context[\"start_span\"].to_list()\n",
    "        spans2 = context[\"end_span\"].to_list()\n",
    "        print(queries[i] + \"\\n\")\n",
    "        for j in range(len(context.index)):\n",
    "            print(f\"Rank {j+1}: \" + str(df[df[\"_id\"] == ix[j]].iloc[0][\"text\"])[spans1[j]:spans2[j]] + \"\\n\")\n",
    "            \n",
    "    ## Cleanup so we're not making giant DataFrames\n",
    "    w2v_searchable_df.drop(columns=[column for column in w2v_searchable_df.columns if column.startswith(\"temp\")],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try it out! \n",
    "\n",
    "Define your own queries, and see what you find. Make sure to use statements and not questions. My friends tried asking it questions, and it came up with some really weird results because I think the really long article has a comments section... \n",
    "\n",
    "Anyway, enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of your statements to find things similar to in the corpus.\n",
    "queries = [\n",
    "    \"Seasonality of transmission of the virus. Times when the virus is transmitted, i.e. winter, summer, autumn, spring, during cold weather, or in different climates.\"\n",
    "]\n",
    "\n",
    "# An integer, the number of top results you want to see.\n",
    "n = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_these(queries=queries, n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max([i.split(\"q\") for i in df[\"text\"]], key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: The to-do list\n",
    "\n",
    "There's still a lot of work to be done. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Implement something that replaces all the initialisms and acronyms with their full word forms.\n",
    "    * See: https://pypi.org/project/scispacy/\n",
    "* Implement entity linking to canonicalize data and increase consistency across vectorized outputs. \n",
    "    * See: UmlsEntityLinker\n",
    "* Change the dictionary parsers to include information like authors, affiliations, and other metadata I initially left out for the sake of simplicity. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
