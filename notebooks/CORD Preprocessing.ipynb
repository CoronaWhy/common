{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "import numpy as np\n",
    "import scispacy\n",
    "import json\n",
    "import spacy\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.spatial import distance\n",
    "import ipywidgets as widgets\n",
    "from scispacy.abbreviation import AbbreviationDetector\n",
    "from spacy_langdetect import LanguageDetector\n",
    "# UMLS linking will find concepts in the text, and link them to UMLS. \n",
    "from scispacy.umls_linking import UmlsEntityLinker\n",
    "import time\n",
    "from spacy.vocab import Vocab\n",
    "from multiprocessing import Process, Queue, Manager\n",
    "from multiprocessing.pool import Pool\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a dictionary object that's easy to parse in pandas.\n",
    "def extract_title_from_json(js):\n",
    "    \n",
    "    # For text mining purposes, we're only interested in 4 columns:\n",
    "    # abstract, paper_id (for ease of indexing), title, and body text.\n",
    "    # In this particular dataset, some abstracts have multiple sections,\n",
    "    # with [\"abstract\"][1] or later representing keywords or extra info. \n",
    "    # We only want to keep [0][\"text\"] in these cases. \n",
    "    json_dict = [\n",
    "            js[\"paper_id\"],\n",
    "            \"title\",\n",
    "            js[\"metadata\"][\"title\"],\n",
    "            ]\n",
    "    return json_dict\n",
    "\n",
    "# Returns a dictionary object that's easy to parse in pandas. For tables! :D\n",
    "def extract_tables_from_json(js):\n",
    "    json_dict_list = []\n",
    "    # Figures contain useful information. Since NLP doesn't handle images and tables,\n",
    "    # we can leverage this text data in lieu of visual data.\n",
    "    for figure in list(js[\"ref_entries\"].keys()):\n",
    "        json_dict = [\n",
    "            js[\"paper_id\"],\n",
    "            figure,\n",
    "            js[\"ref_entries\"][figure][\"text\"]]\n",
    "        json_dict_list.append(json_dict)\n",
    "    return json_dict_list\n",
    "\n",
    "def extract_abstract_from_json(js):\n",
    "    \n",
    "    # In this particular dataset, some abstracts have multiple sections,\n",
    "    # with [\"abstract\"][1] or later representing keywords or extra info. \n",
    "    # We only want to keep [0][\"text\"] in these cases. \n",
    "    if len(js[\"abstract\"]) > 0:\n",
    "        json_dict = [\n",
    "            js[\"paper_id\"],\n",
    "            \"abstract\",\n",
    "            js[\"abstract\"][0][\"text\"]\n",
    "        ]\n",
    "        return json_dict\n",
    "        \n",
    "    # Else, [\"abstract\"] isn't a list and we can just grab the full text.\n",
    "    else:\n",
    "        json_dict = [\n",
    "            js[\"paper_id\"],\n",
    "            \"abstract\",\n",
    "            js[\"abstract\"],\n",
    "        ]\n",
    "\n",
    "        return json_dict\n",
    "\n",
    "# Kudos and thanks to @Imran for creating this amazing iterator <3 <3 <3 \n",
    "\n",
    "class Extraction:\n",
    "    def __init__(self,data_dir='./Cord-2/'):\n",
    "        self.map2file = self.create_map2file(data_dir)\n",
    "    def create_map2file(self,data_dir):\n",
    "        map2file = dict()\n",
    "        for dirname, _, filenames in os.walk(data_dir):\n",
    "            for filename in filenames:\n",
    "                name = filename.split('.')\n",
    "                if len(name) > 1 and name[1] == 'json':\n",
    "                    map2file[name[0]] = os.path.join(dirname, filename)\n",
    "        return map2file\n",
    "    def prep_data(self,file_list=None):\n",
    "        if file_list==None:\n",
    "            files = list(self.map2file)\n",
    "        else:\n",
    "            files = file_list\n",
    "        for file_id in files:\n",
    "            '''\n",
    "            Generator providing section with labels\n",
    "                0  _id  Section_name Text\n",
    "                1\n",
    "                2\n",
    "            '''\n",
    "            past_sec = None\n",
    "            with open(self.map2file[file_id]) as paperjs:\n",
    "                jsfile = json.load(paperjs)\n",
    "                yield extract_title_from_json(jsfile)\n",
    "                yield extract_abstract_from_json(jsfile) \n",
    "                for _,section in enumerate(jsfile['body_text']):\n",
    "                    if past_sec != None and past_sec != section['section']:\n",
    "                        #print('{} and{}'.format(past_sec,section))\n",
    "                        past_sec = section['section']\n",
    "                    yield [file_id,section['section'],section['text']]\n",
    "                tables = extract_tables_from_json(jsfile)\n",
    "                for i in tables: \n",
    "                    yield i\n",
    "                    \n",
    "filter_dict = {\n",
    "    \"discussion\": [\"conclusions\",\"conclusion\",'| discussion', \"discussion\",  'concluding remarks',\n",
    "                   'discussion and conclusions','conclusion:', 'discussion and conclusion',\n",
    "                   'conclusions:', 'outcomes', 'conclusions and perspectives', \n",
    "                   'conclusions and future perspectives', 'conclusions and future directions'],\n",
    "    \"results\": ['executive summary', 'result', 'summary','results','results and discussion','results:',\n",
    "                'comment',\"findings\"],\n",
    "    \"introduction\": ['introduction', 'background', 'i. introduction','supporting information','| introduction'],\n",
    "    \"methods\": ['methods','method','statistical methods','materials','materials and methods',\n",
    "                'data collection','the study','study design','experimental design','objective',\n",
    "                'objectives','procedures','data collection and analysis', 'methodology',\n",
    "                'material and methods','the model','experimental procedures','main text',],\n",
    "    \"statistics\": ['data analysis','statistical analysis', 'analysis','statistical analyses', \n",
    "                   'statistics','data','measures'],\n",
    "    \"clinical\": ['diagnosis', 'diagnostic features', \"differential diagnoses\", 'classical signs','prognosis', 'clinical signs', 'pathogenesis',\n",
    "                 'etiology','differential diagnosis','clinical features', 'case report', 'clinical findings',\n",
    "                 'clinical presentation'],\n",
    "    'treatment': ['treatment', 'interventions'],\n",
    "    \"prevention\": ['epidemiology','risk factors'],\n",
    "    \"subjects\": ['demographics','samples','subjects', 'study population','control','patients', \n",
    "               'participants','patient characteristics'],\n",
    "    \"animals\": ['animals','animal models'],\n",
    "    \"abstract\": [\"abstract\", 'a b s t r a c t','author summary'], \n",
    "    \"review\": ['review','literature review','keywords']}\n",
    "\n",
    "def invert_dict(d): \n",
    "    inverse = dict() \n",
    "    for key in d: \n",
    "        # Go through the list that is saved in the dict:\n",
    "        for item in d[key]:\n",
    "            # Check if in the inverted dict the key exists\n",
    "            if item not in inverse: \n",
    "                # If not create a new list\n",
    "                inverse[item] = [key] \n",
    "            else: \n",
    "                inverse[item].append(key) \n",
    "    return inverse\n",
    "inverted_dict = invert_dict(filter_dict)\n",
    "    \n",
    "def get_section_name(text):\n",
    "    if len(text) == 0:\n",
    "        return(text)\n",
    "    text = text.lower()\n",
    "    if text in inverted_dict.keys():\n",
    "        return(inverted_dict[text][0])\n",
    "    else:\n",
    "        if \"case\" in text or \"study\" in text: \n",
    "            return(\"methods\")\n",
    "        elif \"clinic\" in text:\n",
    "            return(\"clinical\")\n",
    "        elif \"stat\" in text:\n",
    "            return(\"statistics\")\n",
    "        elif \"intro\" in text or \"backg\" in text:\n",
    "            return(\"introduction\")\n",
    "        elif \"data\" in text:\n",
    "            return(\"statistics\")\n",
    "        elif \"discuss\" in text:\n",
    "            return(\"discussion\")\n",
    "        elif \"patient\" in text:\n",
    "            return(\"subjects\")\n",
    "        else: \n",
    "            return(text)\n",
    "\n",
    "def init_nlp():\n",
    "    nlp = spacy.load(\"en_core_sci_lg\", disable=[\"tagger\"])\n",
    "    nlp.max_length=2000000\n",
    "\n",
    "    # We also need to detect language, or else we'll be parsing non-english text \n",
    "    # as if it were English. \n",
    "    nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\n",
    "\n",
    "    # Add the abbreviation pipe to the spacy pipeline. Only need to run this once.\n",
    "    abbreviation_pipe = AbbreviationDetector(nlp)\n",
    "    nlp.add_pipe(abbreviation_pipe)\n",
    "\n",
    "    # Our linker will look up named entities/concepts in the UMLS graph and normalize\n",
    "    # the data for us. \n",
    "    linker = UmlsEntityLinker(resolve_abbreviations=True)\n",
    "    nlp.add_pipe(linker)\n",
    "    \n",
    "    new_vector = nlp(\n",
    "               \"\"\"Positive-sense single‚Äêstranded ribonucleic acid virus, subgenus \n",
    "                   sarbecovirus of the genus Betacoronavirus. \n",
    "                   Also known as severe acute respiratory syndrome coronavirus 2, \n",
    "                   also known by 2019 novel coronavirus. It is \n",
    "                   contagious in humans and is the cause of the ongoing pandemic of \n",
    "                   coronavirus disease. Coronavirus disease 2019 is a zoonotic infectious \n",
    "                   disease.\"\"\").vector\n",
    "\n",
    "    vector_data = {\"COVID-19\": new_vector,\n",
    "               \"2019-nCoV\": new_vector,\n",
    "               \"SARS-CoV-2\": new_vector}\n",
    "\n",
    "    vocab = Vocab()\n",
    "    for word, vector in vector_data.items():\n",
    "        nlp.vocab.set_vector(word, vector)\n",
    "    \n",
    "    return(nlp, linker)\n",
    "def init_ner():\n",
    "    models = [\"en_ner_craft_md\", \"en_ner_jnlpba_md\",\"en_ner_bc5cdr_md\",\"en_ner_bionlp13cg_md\"]\n",
    "    nlps = [spacy.load(model) for model in models]\n",
    "    return(nlps)\n",
    "\n",
    "def gather_everything(data_dir):\n",
    "    ex = Extraction(data_dir=data_dir)\n",
    "    df_iter = ex.prep_data(None) \n",
    "    df_list =[j for j in [i for i in df_iter]]\n",
    "    df = pd.DataFrame(columns=[\"paper_id\",\"section\",\"text\"], data=df_list)\n",
    "    df[\"section\"] = [get_section_name(i) for i in df[\"section\"]]\n",
    "    return(df)\n",
    "\n",
    "def pipeline(df):\n",
    "    languages = []\n",
    "    start_chars = []\n",
    "    end_chars = []\n",
    "    entities = []\n",
    "    sentences = []\n",
    "    lemmas = []\n",
    "    vectors = []\n",
    "    _ids = []\n",
    "    columns = []\n",
    "    \n",
    "    nlp, linker = init_nlp()\n",
    "    nlps = init_ner()\n",
    "    \n",
    "    scispacy_ent_types = ['GGP', 'SO', 'TAXON', 'CHEBI', 'GO', 'CL', 'DNA', 'CELL_TYPE', 'CELL_LINE', 'RNA', 'PROTEIN', \n",
    "                          'DISEASE', 'CHEMICAL', 'CANCER', 'ORGAN', 'TISSUE', 'ORGANISM', 'CELL', 'AMINO_ACID',\n",
    "                          'GENE_OR_GENE_PRODUCT', 'SIMPLE_CHEMICAL', 'ANATOMICAL_SYSTEM', 'IMMATERIAL_ANATOMICAL_ENTITY',\n",
    "                          'MULTI-TISSUE_STRUCTURE', 'DEVELOPING_ANATOMICAL_STRUCTURE', 'ORGANISM_SUBDIVISION',\n",
    "                          'CELLULAR_COMPONENT', 'PATHOLOGICAL_FORMATION']\n",
    "    \n",
    "    for i in tqdm(range(len(df))):\n",
    "        doc = nlp(str(df.iloc[i][\"text\"]))\n",
    "        sents = [sent for sent in doc.sents]\n",
    "\n",
    "        if len(doc._.abbreviations) > 0 and doc._.language[\"language\"] == \"en\":\n",
    "            doc._.abbreviations.sort()\n",
    "            join_list = []\n",
    "            start = 0\n",
    "            for abbrev in doc._.abbreviations:\n",
    "                join_list.append(str(doc.text[start:abbrev.start_char]))\n",
    "                if len(abbrev._.long_form) > 5: #Increase length so \"a\" and \"an\" don't get un-abbreviated\n",
    "                    join_list.append(str(abbrev._.long_form))\n",
    "                else:\n",
    "                    join_list.append(str(doc.text[abbrev.start_char:abbrev.end_char]))\n",
    "                start = abbrev.end_char\n",
    "            # Reassign fixed body text to article in df.\n",
    "            new_text = \"\".join(join_list)\n",
    "            # We have new text. Re-nlp the doc for futher processing!\n",
    "            doc = nlp(new_text)\n",
    "\n",
    "        if doc._.language[\"language\"] == \"en\" and len(doc.text) > 5:\n",
    "            sents = [sent for sent in doc.sents]\n",
    "            for sent in sents:\n",
    "                languages.append(doc._.language[\"language\"])\n",
    "                sentences.append(sent.text)\n",
    "                vectors.append(sent.vector)\n",
    "                lemmas.append([token.lemma_ for token in doc])\n",
    "                doc_ents = []\n",
    "                for ent in sent.ents: \n",
    "                    if len(ent._.umls_ents) > 0:\n",
    "                        poss = linker.umls.cui_to_entity[ent._.umls_ents[0][0]].canonical_name\n",
    "                        doc_ents.append(poss)\n",
    "                entities.append(doc_ents)\n",
    "                _ids.append(df.iloc[i][\"paper_id\"])\n",
    "                columns.append(df.iloc[i][\"section\"])\n",
    "        else: \n",
    "            entities.append(\"[]\")\n",
    "            sentences.append(doc.text)\n",
    "            vectors.append(np.zeros(200))\n",
    "            lemmas.append(\"[]\")\n",
    "            _ids.append(df.iloc[i,0])\n",
    "            languages.append(doc._.language[\"language\"])\n",
    "            columns.append(df.iloc[i][\"section\"])\n",
    "\n",
    "    new_df = pd.DataFrame(data={\"paper_id\": _ids, \"language\": languages,\n",
    "                                \"section\": columns, \"sentence\": sentences,\n",
    "                                \"lemma\": lemmas, \"UMLS\": entities, \"w2vVector\": vectors})\n",
    "    for col in scispacy_ent_types:\n",
    "        new_df[col] = \"[]\"\n",
    "    for j in tqdm(new_df.index):\n",
    "        if new_df.iloc[j][\"language\"] == \"en\":\n",
    "            for nlp in nlps:\n",
    "                doc = nlp(str(new_df.iloc[j][\"sentence\"]))\n",
    "                keys = list(set([ent.label_ for ent in doc.ents]))\n",
    "                for key in keys:\n",
    "\n",
    "                    # Some entity types are present in the model, but not in the documentation! \n",
    "                    # In that case, we'll just automatically add it to the df. \n",
    "                    if key not in scispacy_ent_types:\n",
    "                        new_df = pd.concat([new_df,pd.DataFrame(columns=[key])])\n",
    "                        new_df[key] = \"[]\"\n",
    "\n",
    "                    values = [ent.text for ent in doc.ents if ent.label_ == key]\n",
    "                    new_df.at[j,key] = values\n",
    "                    \n",
    "    new_df.to_csv(\"df_parts/\" + new_df.iloc[0][\"paper_id\"] + \".complete\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gather_everything(\"CORD-19-research-challenge/\")\n",
    "df.to_csv(\"dataset_v6.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df_parts/dataset_v6.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize_dataframe(df, func, n_cores=6):\n",
    "    df_split = np.array_split(df, 100)[19:]\n",
    "    pool = Pool(n_cores)\n",
    "    list(tqdm(pool.imap_unordered(func, df_split), total=len(df_split)))\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallelize_dataframe(df, pipeline, n_cores=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walks all subdirectories in a directory, and their files. \n",
    "# Opens all json files we deem relevant, and append them to\n",
    "# a list that can be used as the \"data\" argument in a call to \n",
    "# pd.DataFrame.\n",
    "def gather_jsons(dirName):\n",
    "    \n",
    "    # Get the list of all files in directory tree at given path\n",
    "    # include only json with encoded id (40-character SHA hash)\n",
    "    # Only length of filename is checked, but this should be sufficient\n",
    "    # given the task.\n",
    "    \n",
    "    listOfFiles = list()\n",
    "    for (dirpath, dirnames, filenames) in os.walk(dirName):\n",
    "        listOfFiles += [os.path.join(dirpath, file) for file in filenames\n",
    "                        if file.endswith(\"json\")\n",
    "                        and len(file) == 45]\n",
    "    jsons = []\n",
    "    \n",
    "    print(str(len(listOfFiles)) + \" jsons found! Attempting to gather.\")\n",
    "    \n",
    "    for file in tqdm(listOfFiles):\n",
    "        with open(file) as json_file:\n",
    "            jsons.append(json.load(json_file))\n",
    "    return jsons\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>section</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dear editor,</th>\n",
       "      <td>366</td>\n",
       "      <td>366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>necropsy findings</th>\n",
       "      <td>365</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>western blotting</th>\n",
       "      <td>362</td>\n",
       "      <td>362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.</th>\n",
       "      <td>356</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>figref29</th>\n",
       "      <td>354</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pathology</th>\n",
       "      <td>353</td>\n",
       "      <td>353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contents lists available at sciencedirect</th>\n",
       "      <td>352</td>\n",
       "      <td>352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>future directions</th>\n",
       "      <td>349</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plasmid construction</th>\n",
       "      <td>347</td>\n",
       "      <td>347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>345</td>\n",
       "      <td>345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>examen clinique et traitement</th>\n",
       "      <td>344</td>\n",
       "      <td>344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tabref17</th>\n",
       "      <td>343</td>\n",
       "      <td>343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>materiali e metodi</th>\n",
       "      <td>343</td>\n",
       "      <td>343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>figref30</th>\n",
       "      <td>338</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>electron microscopy</th>\n",
       "      <td>333</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disclosures</th>\n",
       "      <td>327</td>\n",
       "      <td>327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>figref31</th>\n",
       "      <td>325</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elisa</th>\n",
       "      <td>321</td>\n",
       "      <td>321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recommendations</th>\n",
       "      <td>319</td>\n",
       "      <td>319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perspectives</th>\n",
       "      <td>317</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>316</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laboratory animal medicine</th>\n",
       "      <td>316</td>\n",
       "      <td>316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clinique</th>\n",
       "      <td>315</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>definitions</th>\n",
       "      <td>315</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>figref32</th>\n",
       "      <td>314</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>serology</th>\n",
       "      <td>312</td>\n",
       "      <td>312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>findings</th>\n",
       "      <td>310</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>highlights</th>\n",
       "      <td>310</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tuberculosis</th>\n",
       "      <td>309</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tabref18</th>\n",
       "      <td>306</td>\n",
       "      <td>306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sensitivity analysis</th>\n",
       "      <td>302</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>figref33</th>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sequence analysis</th>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>immunohistochemistry</th>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>antibodies</th>\n",
       "      <td>300</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transmission</th>\n",
       "      <td>298</td>\n",
       "      <td>298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pathophysiology</th>\n",
       "      <td>297</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>histopathology</th>\n",
       "      <td>295</td>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>species</th>\n",
       "      <td>295</td>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>patients and methods</th>\n",
       "      <td>293</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plasmids</th>\n",
       "      <td>292</td>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>virus isolation</th>\n",
       "      <td>291</td>\n",
       "      <td>291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vaccination</th>\n",
       "      <td>291</td>\n",
       "      <td>291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>figref34</th>\n",
       "      <td>286</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ethical approval</th>\n",
       "      <td>282</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>general aspects</th>\n",
       "      <td>280</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tabref19</th>\n",
       "      <td>279</td>\n",
       "      <td>279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>journal of medicinal chemistry</th>\n",
       "      <td>278</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cells</th>\n",
       "      <td>274</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>figref35</th>\n",
       "      <td>273</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           paper_id  text\n",
       "section                                                  \n",
       "dear editor,                                    366   366\n",
       "necropsy findings                               365   365\n",
       "western blotting                                362   362\n",
       "3.                                              356   356\n",
       "figref29                                        354   354\n",
       "pathology                                       353   353\n",
       "contents lists available at sciencedirect       352   352\n",
       "future directions                               349   349\n",
       "plasmid construction                            347   347\n",
       "model                                           345   345\n",
       "examen clinique et traitement                   344   344\n",
       "tabref17                                        343   343\n",
       "materiali e metodi                              343   343\n",
       "figref30                                        338   338\n",
       "electron microscopy                             333   333\n",
       "disclosures                                     327   327\n",
       "figref31                                        325   325\n",
       "elisa                                           321   321\n",
       "recommendations                                 319   319\n",
       "perspectives                                    317   317\n",
       "1                                               316   316\n",
       "laboratory animal medicine                      316   316\n",
       "clinique                                        315   315\n",
       "definitions                                     315   315\n",
       "figref32                                        314   314\n",
       "serology                                        312   312\n",
       "findings                                        310   310\n",
       "highlights                                      310   310\n",
       "tuberculosis                                    309   309\n",
       "tabref18                                        306   306\n",
       "sensitivity analysis                            302   302\n",
       "figref33                                        300   300\n",
       "sequence analysis                               300   300\n",
       "immunohistochemistry                            300   300\n",
       "antibodies                                      300   300\n",
       "transmission                                    298   298\n",
       "pathophysiology                                 297   297\n",
       "histopathology                                  295   295\n",
       "species                                         295   295\n",
       "patients and methods                            293   293\n",
       "plasmids                                        292   292\n",
       "virus isolation                                 291   291\n",
       "vaccination                                     291   291\n",
       "figref34                                        286   286\n",
       "ethical approval                                282   282\n",
       "general aspects                                 280   280\n",
       "tabref19                                        279   279\n",
       "journal of medicinal chemistry                  278   278\n",
       "cells                                           274   274\n",
       "figref35                                        273   273"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"section\").count().sort_values(by=\"paper_id\",ascending=False)[100:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
