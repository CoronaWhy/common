{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentence.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNoaZtjyzx58/aVqaaAef5u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isaacmg/common/blob/sentence_notebook/Sentence%20Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlU8oDkouvXT",
        "colab_type": "text"
      },
      "source": [
        "# Training Sentence Transformers for COVID-19\n",
        "This is a basic notebook to train sentence transformers on the MedNLI and SciTail datasets. This notebook was run on Google Colaboratory Pro using a single GPU. For a more general tutorial on why [sentence embeddings might be useful see here](https://www.kaggle.com/isaacmg/scibert-embeddings). Training times were generally less than two hours (except when training on AllNLI). The finalized weights can be found on Kaggle (see link in Coronawhy). Note since there is no way to validate results concretely at the moment there are several different weights you can use. Sorry for the somewhat messy code as I only had access to GPUs on colab I didn't bother refactoring into scripts and methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DojM55a9nwt",
        "colab_type": "code",
        "outputId": "bb70a5d7-a6ac-4f94-ce4f-9cb101d9a19b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "!git clone https://github.com/UKPLab/sentence-transformers\n",
        "import os \n",
        "import json\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'sentence-transformers'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 729 (delta 8), reused 3 (delta 0), pack-reused 714\u001b[K\n",
            "Receiving objects: 100% (729/729), 208.14 KiB | 379.00 KiB/s, done.\n",
            "Resolving deltas: 100% (490/490), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqo4fw8Q-N9d",
        "colab_type": "code",
        "outputId": "a8f072c3-99ab-44c0-84f5-b336bfe5ccd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "os.chdir('sentence-transformers')\n",
        "!pip install -r requirements.txt\n",
        "!python examples/datasets/get_data.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extract wikipedia-sections-triplets.zip\n",
            "All datasets downloaded and extracted\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M__sOXZ9-aOD",
        "colab_type": "code",
        "outputId": "a5647277-b083-4e96-8192-e51758933fee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import math\n",
        "from sentence_transformers import models, losses\n",
        "from sentence_transformers import SentencesDataset, LoggingHandler, SentenceTransformer\n",
        "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
        "from sentence_transformers.readers import *\n",
        "import logging\n",
        "from datetime import datetime\n",
        "\n",
        "#### Just some code to print debug information to stdout\n",
        "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
        "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
        "                    level=logging.INFO,\n",
        "                    handlers=[LoggingHandler()])\n",
        "# Download relevant data to directories\n",
        "!mkdir data \n",
        "!gsutil cp gs://qa_research/scitail_1.0_train.txt data/scitail_1.0_train.jsonl\n",
        "!gsutil cp gs://qa_research/mli_train_v1.jsonl data/mli_train_v1.jsonl"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://qa_research/scitail_1.0_train.txt...\n",
            "\\ [1 files][ 27.2 MiB/ 27.2 MiB]                                                \n",
            "Operation completed over 1 objects/27.2 MiB.                                     \n",
            "Copying gs://qa_research/mli_train_v1.jsonl...\n",
            "- [1 files][ 10.5 MiB/ 10.5 MiB]                                                \n",
            "Operation completed over 1 objects/10.5 MiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unj85L1BwsJE",
        "colab_type": "text"
      },
      "source": [
        "## Pre-training on AllNLI\n",
        "Even though the AllNI dataset isn't scientific it can still form the basis of useful pre-training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOMv_MGQN8bG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will use the recent model by deepset as the base LM\n",
        "model_name = \"deepset/covid_bert_base\"\n",
        "model_save_path = 'pretrained_'+model_name+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "# Use BERT for mapping tokens to embeddings\n",
        "word_embedding_model = models.BERT(model_name)\n",
        "\n",
        "# Apply mean pooling to get one fixed sized sentence vector\n",
        "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
        "                               pooling_mode_mean_tokens=True,\n",
        "                               pooling_mode_cls_token=False,\n",
        "                               pooling_mode_max_tokens=False)\n",
        "\n",
        "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
        "\n",
        "\n",
        "# Initialize the AllNLI training data\n",
        "logging.info(\"Read AllNLI train dataset\")\n",
        "batch_size = 16\n",
        "nli_reader = NLIDataReader(\"examples/datasets/AllNLI\")\n",
        "train_data_nli = SentencesDataset(nli_reader.get_examples('train.gz'), model=model)\n",
        "train_dataloader_nli = DataLoader(train_data_nli, shuffle=True, batch_size=batch_size)\n",
        "train_loss_nli = losses.SoftmaxLoss(model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels=nli_reader.get_num_labels())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpXBajQdzfqP",
        "colab_type": "code",
        "outputId": "51ed0854-8af6-44b6-ae98-348179ce2660",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# We will use STSDataReader for all of the evaluation \n",
        "sts_reader = STSDataReader('examples/datasets/stsbenchmark')\n",
        "dev_data = SentencesDataset(examples=sts_reader.get_examples('sts-dev.csv'), model=model)\n",
        "dev_dataloader = DataLoader(dev_data, shuffle=False, batch_size=batch_size)\n",
        "evaluator = EmbeddingSimilarityEvaluator(dev_dataloader)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Convert dataset: 100%|██████████| 1500/1500 [00:00<00:00, 1520.96it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-03-30 19:19:23 - Num sentences: 1500\n",
            "2020-03-30 19:19:23 - Sentences 0 longer than max_seqence_length: 0\n",
            "2020-03-30 19:19:23 - Sentences 1 longer than max_seqence_length: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcWVAgjEzkD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "warmup_steps = math.ceil(len(train_dataloader_nli) * 1 / batch_size * 0.1)\n",
        "model.fit(train_objectives=[(train_dataloader_nli, train_loss_nli)],\n",
        "          evaluator=evaluator,\n",
        "          epochs=1,\n",
        "          evaluation_steps=1000,\n",
        "          warmup_steps=warmup_steps,\n",
        "          output_path=model_save_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u54TfDAHzqhx",
        "colab_type": "text"
      },
      "source": [
        "## Part II Training on scientific datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhCwab_7F1f0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from . import InputExample\n",
        "import csv\n",
        "import gzip\n",
        "import os\n",
        "\n",
        "class MedNLIReader(object):\n",
        "    \"\"\"\n",
        "    Reads in the Stanford NLI dataset and the MultiGenre NLI dataset\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset_folder):\n",
        "        self.json_path = dataset_folder\n",
        "\n",
        "    def get_examples(self, filename, max_examples=0):\n",
        "        \"\"\"\n",
        "        data_splits specified which data split to use (train, dev, test).\n",
        "        Expects that self.dataset_folder contains the files s1.$data_split.gz,  s2.$data_split.gz,\n",
        "        labels.$data_split.gz, e.g., for the train split, s1.train.gz, s2.train.gz, labels.train.gz\n",
        "        \"\"\"\n",
        "\n",
        "        examples = []\n",
        "        id = 0\n",
        "        with open(self.json_path) as y:\n",
        "          for line in y:\n",
        "            line_final = json.loads(line)\n",
        "            sentence_a = line_final[\"sentence1\"]\n",
        "            sentence_b = line_final[\"sentence2\"]\n",
        "            label = line_final[\"gold_label\"]\n",
        "            guid = \"%s-%d\" % (filename, id)\n",
        "            id += 1\n",
        "            examples.append(InputExample(guid=guid, texts=[sentence_a, sentence_b], label=self.map_label(label)))\n",
        "\n",
        "            if 0 < max_examples <= len(examples):\n",
        "                break\n",
        "\n",
        "        return examples\n",
        "\n",
        "    @staticmethod\n",
        "    def get_labels():\n",
        "        return {\"contradiction\": 0, \"entailment\": 1, \"neutral\": 2}\n",
        "\n",
        "    def get_num_labels(self):\n",
        "        return len(self.get_labels())\n",
        "\n",
        "    def map_label(self, label):\n",
        "        return self.get_labels()[label.strip().lower()]\n",
        "# Read the dataset\n",
        "nli_reader = MedNLIReader('data/scitail_1.0_train.jsonl')\n",
        "nli_reader_mednli = MedNLIReader('data/mli_train_v1.jsonl')\n",
        "train_num_labels = nli_reader.get_num_labels()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlcJihXGMAiT",
        "colab_type": "text"
      },
      "source": [
        "## Train on SCI-Tail"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glNlbJaBDCai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert the dataset to a DataLoader ready for training\n",
        "logging.info(\"Read SCITail Dataset\")\n",
        "train_data = SentencesDataset(nli_reader.get_examples('scitail_1.0_train.jsonl'), model=model)\n",
        "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "train_loss = losses.SoftmaxLoss(model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels=train_num_labels)\n",
        "logging.info(\"Read STSbenchmark dev dataset\")\n",
        "\n",
        "\n",
        "# Configure the training\n",
        "num_epochs = 2\n",
        "\n",
        "warmup_steps = math.ceil(len(train_dataloader) * num_epochs / batch_size * 0.1) #10% of train data for warm-up\n",
        "logging.info(\"Warmup-steps: {}\".format(warmup_steps))\n",
        "\n",
        "\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
        "          evaluator=evaluator,\n",
        "          epochs=num_epochs,\n",
        "          evaluation_steps=1000,\n",
        "          warmup_steps=warmup_steps,\n",
        "          output_path=model_save_path\n",
        "          )\n",
        "\n",
        "\n",
        "\n",
        "##############################################################################\n",
        "#\n",
        "# Load the stored model and evaluate its performance on STS benchmark dataset\n",
        "#\n",
        "##############################################################################\n",
        "\n",
        "model = SentenceTransformer(model_save_path)\n",
        "test_data = SentencesDataset(examples=sts_reader.get_examples(\"sts-test.csv\"), model=model)\n",
        "test_dataloader = DataLoader(test_data, shuffle=False, batch_size=batch_size)\n",
        "evaluator = EmbeddingSimilarityEvaluator(test_dataloader)\n",
        "\n",
        "model.evaluate(evaluator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2q1tlvEFMF-Q",
        "colab_type": "text"
      },
      "source": [
        "## Train on Med-NLI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_PSukdVnX-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logging.info(\"Read MedNLI Dataset\")\n",
        "train_data_med = SentencesDataset(nli_reader_mednli.get_examples('mli_train_v1.jsonl'), model=model)\n",
        "train_dataloader_med = DataLoader(train_data_med, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkabjwQXnU-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_num_labels_med = nli_reader_mednli.get_num_labels()\n",
        "train_loss_med = losses.SoftmaxLoss(model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels=train_num_labels_med)\n",
        "model.fit(train_objectives=[(train_dataloader_med, train_loss_med)],\n",
        "          evaluator=evaluator,\n",
        "          epochs=num_epochs,\n",
        "          evaluation_steps=1000,\n",
        "          warmup_steps=warmup_steps,\n",
        "          output_path=\"model_results2\"\n",
        "          )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--R1eZwUn_Uv",
        "colab_type": "code",
        "outputId": "cdd2b033-0070-4313-b0c0-a9de7dd31da6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "source": [
        "!gsutil cp -r model_results2 gs://coronaviruspublicdata/today/model_results"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://model_results2/modules.json [Content-Type=application/json]...\n",
            "Copying file://model_results2/similarity_evaluation_results.csv [Content-Type=text/csv]...\n",
            "Copying file://model_results2/config.json [Content-Type=application/json]...\n",
            "Copying file://model_results2/1_Pooling/config.json [Content-Type=application/json]...\n",
            "\\\n",
            "==> NOTE: You are performing a sequence of gsutil operations that may\n",
            "run significantly faster if you instead use gsutil -m cp ... Please\n",
            "see the -m section under \"gsutil help options\" for further information\n",
            "about when gsutil -m can be advantageous.\n",
            "\n",
            "Copying file://model_results2/0_BERT/sentence_bert_config.json [Content-Type=application/json]...\n",
            "Copying file://model_results2/0_BERT/added_tokens.json [Content-Type=application/json]...\n",
            "Copying file://model_results2/0_BERT/pytorch_model.bin [Content-Type=application/octet-stream]...\n",
            "==> NOTE: You are uploading one or more large file(s), which would run\n",
            "significantly faster if you enable parallel composite uploads. This\n",
            "feature can be enabled by editing the\n",
            "\"parallel_composite_upload_threshold\" value in your .boto\n",
            "configuration file. However, note that if you do this large files will\n",
            "be uploaded as `composite objects\n",
            "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
            "means that any user who downloads such objects will need to have a\n",
            "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
            "without a compiled crcmod, computing checksums on composite objects is\n",
            "so slow that gsutil disables downloads of composite objects.\n",
            "\n",
            "Copying file://model_results2/0_BERT/vocab.txt [Content-Type=text/plain]...\n",
            "Copying file://model_results2/0_BERT/tokenizer_config.json [Content-Type=application/json]...\n",
            "Copying file://model_results2/0_BERT/config.json [Content-Type=application/json]...\n",
            "Copying file://model_results2/0_BERT/special_tokens_map.json [Content-Type=application/json]...\n",
            "/ [11 files][417.9 MiB/417.9 MiB]    4.3 MiB/s                                  \n",
            "Operation completed over 11 objects/417.9 MiB.                                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXogImQpof5p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gsutil cp -r model_results2 gs://coronaviruspublicdata"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALmaEBNM9A-8",
        "colab_type": "text"
      },
      "source": [
        "## Qualitative Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RcfpBJhu4Ny",
        "colab_type": "code",
        "outputId": "758e7099-a61c-4914-9a1e-31d7ff10eaa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "#!pip install scikit_learn\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "sents = model.encode([\"The SARS 3C-like proteinase (SARS-3CLpro), which is the main proteinase of the SARS coronavirus, is essential to the virus life cycle. This enzyme has been shown to be active as a dimer in which only one protomer is active. However, it remains unknown how the dimer structure maintains an active monomer conformation. It has been observed that the Ser139-Leu141 loop forms a short 3 10 -helix that disrupts the catalytic machinery in the inactive monomer structure. We have tried to disrupt this helical conformation by mutating L141 to T in the stable inactive monomer G11A/R298A/Q299A. The resulting tetra-mutant G11A/L141T/R298A/Q299A is indeed enzymatically active as a monomer. Molecular dynamics simulations revealed that the L141T mutation disrupts the 3 10 -helix and helps to stabilize the active conformation. The coil-3 10 -helix conformational transition of the Ser139-Leu141 loop serves as an enzyme activity switch. Our study therefore indicates that the dimer structure can stabilize the active conformation but is not a required structure in the evolution of the active enzyme, which can also arise through simple mutations\", \"DNA sequences seen in the normal character-based representation appear to have a formidable mixing of the four nucleotides without any apparent order. Nucleotide frequencies and distributions in the sequences have been studied extensively, since the simple rule given by Chargaff almost a century ago that equates the total number of purines to the pyrimidines in a duplex DNA sequence. While it is difficult to trace any relationship between the bases from studies in the character representation of a DNA sequence, graphical representations may provide a clue. These novel representations of DNA sequences have been useful in providing an overview of base distribution and composition of the sequences and providing insights into many hidden structures. We report here our observation based on a graphical representation that the intra-purine and intra-pyrimidine differences in sequences of conserved genes generally follow a quadratic distribution relationship and show that this may have arisen from mutations in the sequences over evolutionary time scales. From this hitherto undescribed relationship for the gene sequences considered in this report we hypothesize that such relationships may be characteristic of these sequences and therefore could become a barrier to large scale sequence alterations that override such characteristics, perhaps through some monitoring process inbuilt in the DNA sequences. Such relationship also raises the possibility of intron sequences playing an important role in maintaining the characteristics and could be indicative of possible intron-late phenomena\"])\n",
        "#query = model.encode([\"chrlorquine COVID-2019 treatment\"])\n",
        "cosine_similarity(np.expand_dims(sents[0], axis=0), np.expand_dims(sents[1], axis=0))\n",
        "#scipy.spatial.distance.cdist(sents[0], sents[1], \"cosine\")[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 22.78it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.825353]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pChUvycwEDuW",
        "colab_type": "code",
        "outputId": "3b93e6d8-8f11-44f1-dd9b-cd51cb3a4303",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "sents = model.encode([\"coronavirus\", \"mers\", \"sars\", \"covid-19\", \"dog\"])\n",
        "print(cosine_similarity(np.expand_dims(sents[0], axis=0), np.expand_dims(sents[1], axis=0)))\n",
        "print(cosine_similarity(np.expand_dims(sents[0], axis=0), np.expand_dims(sents[2], axis=0)))\n",
        "print(cosine_similarity(np.expand_dims(sents[0], axis=0), np.expand_dims(sents[3], axis=0)))\n",
        "print(cosine_similarity(np.expand_dims(sents[0], axis=0), np.expand_dims(sents[4], axis=0)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.88it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[0.39761117]]\n",
            "[[0.6937201]]\n",
            "[[0.6606833]]\n",
            "[[0.1408183]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMePqr2DvrHq",
        "colab_type": "code",
        "outputId": "a7dc6d50-bbf9-4eea-cfb9-cd7298a7d2c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "#!pip install scikit_learn\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "sents = model.encode([\"Dogs eating food\", \"coronavirus treatment\", \"chrlorquine COVID-2019 treatment\"])\n",
        "cosine_similarity(np.expand_dims(sents[1], axis=0), np.expand_dims(sents[2], axis=0))\n",
        "#scipy.spatial.distance.cdist(sents[0], sents[1], \"cosine\")[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.80it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.84475654]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adVNJ9P15HaJ",
        "colab_type": "code",
        "outputId": "8bc64be9-ff34-4e68-f05b-ca8ab8bd51ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "new_sents = model.encode([\"treatment efffiacy of chrlorquine on COVID patients\", \"bat to human transmission mechanism of coronaviruses\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.25it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLJCiNRnKjVa",
        "colab_type": "code",
        "outputId": "ea946b87-46e8-46db-fba5-78bdaeb28080",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "cosine_similarity(np.expand_dims(new_sents[0], axis=0), np.expand_dims(new_sents[1], axis=0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5759958]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwlQ41mOKvMC",
        "colab_type": "code",
        "outputId": "b2b354b5-ea33-4b31-915c-a691ed3c6405",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "new_sents = model.encode([\"bat to human transmission mechanism of coronaviruses\", \"camel to human transmission mechanism of middle east respiratory viruses\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.36it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HH1FS5rK3gJ",
        "colab_type": "code",
        "outputId": "03378cf8-be92-4159-a15b-65bcfcfa5438",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "cosine_similarity(np.expand_dims(new_sents[0], axis=0), np.expand_dims(new_sents[1], axis=0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.8909222]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrr1BjhmK4Qa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}